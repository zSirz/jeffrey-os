#!/bin/bash
# Script d'installation et configuration d'Ollama pour Jeffrey OS

set -euo pipefail

echo "üöÄ Installing and configuring Ollama for Jeffrey OS"

OS=$(uname -s)

# Installation d'Ollama selon l'OS
if [[ "$OS" == "Darwin" ]]; then
    echo "üçé macOS detected"

    # V√©rifier si Ollama est d√©j√† install√©
    if command -v ollama &>/dev/null; then
        echo "‚úÖ Ollama is already installed"
    else
        # Pr√©f√©rer Homebrew si disponible
        if command -v brew &>/dev/null; then
            echo "üì¶ Installing Ollama via Homebrew..."
            brew install ollama
        else
            echo "üì¶ Installing Ollama via official script..."
            curl -fsSL https://ollama.com/install.sh | sh
        fi
    fi
else
    echo "üêß Linux detected"

    if command -v ollama &>/dev/null; then
        echo "‚úÖ Ollama is already installed"
    else
        echo "üì¶ Installing Ollama via official script..."
        curl -fsSL https://ollama.com/install.sh | sh
    fi
fi

# D√©marrer le serveur Ollama
echo "üöÄ Starting Ollama server..."
if ! pgrep -x "ollama" >/dev/null; then
    ollama serve &
    OLLAMA_PID=$!
    echo "‚è≥ Waiting for Ollama to be ready..."
    sleep 5
else
    echo "‚úÖ Ollama server is already running"
fi

# Fonction pour t√©l√©charger un mod√®le
download_model() {
    local model=$1
    echo "üì¶ Downloading model: $model"
    if ollama list | grep -q "$model"; then
        echo "‚úÖ Model $model already downloaded"
    else
        ollama pull "$model"
    fi
}

# T√©l√©charger les mod√®les recommand√©s
echo ""
echo "üì¶ Downloading recommended models..."
echo "This may take a few minutes on first run..."

# Mistral 7B - Excellent pour le fran√ßais
download_model "mistral:7b-instruct"

# Optionnel : mod√®les suppl√©mentaires (comment√©s par d√©faut)
echo ""
echo "Optional models (uncomment to download):"
echo "  # download_model 'llama3.2:3b'  # Llama 3.2 - Plus r√©cent, multilingue"
echo "  # download_model 'phi3:medium'   # Phi-3 - L√©ger et rapide"

# Tester que tout fonctionne
echo ""
echo "üß™ Testing Ollama installation..."
echo "Test query" | ollama run mistral:7b-instruct "R√©ponds simplement 'OK' si tu fonctionnes." || {
    echo "‚ùå Test failed. Please check Ollama installation."
    exit 1
}

echo ""
echo "‚ú® Ollama is ready!"
echo ""
echo "üìä Installed models:"
ollama list

echo ""
echo "üéØ Next steps:"
echo "  1. Start the LLM server: ./scripts/start_llm_server.sh"
echo "  2. Run tests: python tests/test_llm_adaptive.py"
echo "  3. Start Jeffrey OS: python main.py"
echo ""
echo "üí° Tip: Ollama will use the OpenAI-compatible API on port 9010 via our proxy"
