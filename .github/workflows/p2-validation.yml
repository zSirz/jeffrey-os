name: Jeffrey OS P2 Validation

on:
  push:
    branches: [main, p2-*]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

env:
  PYTHON_VERSION: '3.11'

jobs:
  audit:
    name: Code Audit
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run P1 Audit
        run: |
          python scripts/audit_p1.py

      - name: Upload audit report
        uses: actions/upload-artifact@v3
        with:
          name: audit-report
          path: audit_p1_report.json

  test:
    name: Tests and Quality
    runs-on: ubuntu-latest
    needs: audit

    services:
      nats:
        image: nats:2.10-alpine
        ports:
          - 4222:4222
        options: --health-cmd "wget -qO- http://localhost:8222/healthz"

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: --health-cmd "redis-cli ping"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements-p2.txt

      - name: Run linting
        run: |
          flake8 src/ --max-line-length=120
          mypy src/ --ignore-missing-imports

      - name: Run security scan
        run: |
          bandit -r src/ -ll
          safety check

      - name: Run tests
        env:
          NATS_URL: nats://localhost:4222
          REDIS_HOST: localhost
        run: |
          pytest tests/ -v --cov=src/jeffrey --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml

  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push'

    steps:
      - uses: actions/checkout@v4

      - name: Run benchmarks
        run: |
          make benchmark

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark_results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
