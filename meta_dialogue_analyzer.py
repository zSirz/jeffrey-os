"""
MetaDialogueAnalyzer - Analyseur intelligent de qualit√© conversationnelle pour Jeffrey V1.1
√âvalue et am√©liore la qualit√© des conversations en temps r√©el
Fusion des meilleures id√©es de Claude, ChatGPT/Marc et Grok
"""

import math
import re
import statistics
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from .robust_error_handler import jeffrey_error_handler, robust


@dataclass
class ConversationMetrics:
    """M√©triques de qualit√© d'une conversation"""

    engagement_score: float = 0.0  # Score d'engagement (0-1)
    coherence_score: float = 0.0  # Coh√©rence du dialogue (0-1)
    depth_score: float = 0.0  # Profondeur des √©changes (0-1)
    naturalness_score: float = 0.0  # Naturel de la conversation (0-1)
    emotional_resonance: float = 0.0  # R√©sonance √©motionnelle (0-1)
    topic_continuity: float = 0.0  # Continuit√© des sujets (0-1)
    response_quality: float = 0.0  # Qualit√© des r√©ponses (0-1)
    user_satisfaction_estimate: float = 0.0  # Estimation satisfaction (0-1)
    conversation_flow: float = 0.0  # Fluidit√© de la conversation (0-1)
    creativity_level: float = 0.0  # Niveau de cr√©ativit√© (0-1)


@dataclass
class DialogueTurn:
    """Repr√©sente un tour de parole dans le dialogue"""

    timestamp: datetime
    speaker: str  # 'user' ou 'jeffrey'
    message: str
    message_length: int
    emotional_tone: str
    topics: list[str]
    response_time: float | None = None
    engagement_indicators: list[str] = field(default_factory=list)
    quality_indicators: dict[str, float] = field(default_factory=dict)


@dataclass
class ConversationAnalysis:
    """R√©sultat complet d'analyse conversationnelle"""

    conversation_id: str
    metrics: ConversationMetrics
    strengths: list[str]
    weaknesses: list[str]
    suggestions: list[str]
    overall_quality: str  # 'excellent', 'good', 'fair', 'poor'
    detailed_analysis: dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)


class TopicTracker:
    """Traqueur de sujets de conversation"""

    def __init__(self):
        self.topic_transitions = []
        self.current_topics = set()
        self.topic_depth_scores = {}

    @robust("topic_extraction")
    def extract_topics(self, text: str) -> list[str]:
        """Extrait les sujets principaux d'un texte"""
        # Dictionnaire de sujets avec mots-cl√©s
        topic_keywords = {
            'technologie': [
                'tech',
                'ordinateur',
                'ia',
                'intelligence',
                'artificielle',
                'robot',
                'digital',
                'num√©rique',
            ],
            '√©motions': ['sentiment', '√©motion', 'ressens', 'c≈ìur', 'amour', 'joie', 'tristesse', 'peur'],
            'philosophie': ['existence', 'sens', 'pourquoi', 'v√©rit√©', 'conscience', '√™tre', 'penser', 'r√©fl√©chir'],
            'cr√©ativit√©': ['cr√©er', 'art', 'musique', 'peinture', '√©crire', 'imaginer', 'inspiration', 'cr√©atif'],
            'relations': ['ami', 'famille', 'relation', 'social', 'ensemble', 'partager', 'communiquer'],
            'apprentissage': ['apprendre', '√©tude', 'comprendre', 'd√©couvrir', 'connaissance', 'savoir'],
            'nature': ['nature', 'animal', 'plante', 'environnement', '√©cologie', 'terre', 'mer', 'ciel'],
            'temps': ['temps', 'futur', 'pass√©', 'maintenant', 'demain', 'hier', '√©poque', 'moment'],
            'r√™ves': ['r√™ve', 'dormir', 'sommeil', 'songe', 'vision', 'imaginaire', 'fantaisie'],
            'voyage': ['voyage', 'partir', 'd√©couverte', 'explorer', 'aventure', 'pays', 'monde'],
        }

        text_lower = text.lower()
        detected_topics = []

        for topic, keywords in topic_keywords.items():
            keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
            if keyword_count > 0:
                # Score bas√© sur le nombre de mots-cl√©s et leur fr√©quence
                score = keyword_count / len(keywords)
                if score > 0.1:  # Seuil minimal
                    detected_topics.append(topic)

        # Ajouter des sujets sp√©cifiques d√©tect√©s par patterns
        specific_patterns = {
            'jeffrey_identity': r'\b(qui es-tu|ton nom|jeffrey|identit√©)\b',
            'user_questions': r'\?(.*?)(\.|$)',  # Questions
            'personal_sharing': r'\b(je|mon|ma|mes)\b.*\b(vie|histoire|secret|personnel)\b',
        }

        for topic, pattern in specific_patterns.items():
            if re.search(pattern, text_lower):
                detected_topics.append(topic)

        return detected_topics

    @robust("topic_transition")
    def track_topic_transition(self, previous_topics: list[str], current_topics: list[str]) -> float:
        """√âvalue la qualit√© de la transition entre sujets"""
        if not previous_topics:
            return 1.0  # Premi√®re intervention, transition parfaite

        # Calculer l'overlap entre les sujets
        prev_set = set(previous_topics)
        curr_set = set(current_topics)

        intersection = len(prev_set & curr_set)
        union = len(prev_set | curr_set)

        if union == 0:
            return 0.5  # Transition neutre

        # Score de continuit√© (overlap) vs nouveaut√©
        continuity_score = intersection / len(prev_set) if prev_set else 0
        novelty_score = len(curr_set - prev_set) / len(curr_set) if curr_set else 0

        # √âquilibre optimal entre continuit√© et nouveaut√©
        balance_score = 1 - abs(continuity_score - 0.3)  # Optimal ~30% de continuit√©

        return max(0.0, min(1.0, balance_score))


class EngagementAnalyzer:
    """Analyseur d'engagement conversationnel"""

    def __init__(self):
        self.engagement_patterns = self._load_engagement_patterns()

    def _load_engagement_patterns(self) -> dict[str, list[str]]:
        """Charge les patterns d'engagement"""
        return {
            'high_engagement': [
                'fascinating',
                'incroyable',
                'g√©nial',
                'wow',
                'super',
                'adore',
                'continue',
                'dis-moi plus',
                'raconte',
                'comment',
                'pourquoi',
                'j\'aimerais savoir',
                'peux-tu expliquer',
                'tr√®s int√©ressant',
            ],
            'medium_engagement': [
                'ok',
                'd\'accord',
                'bien',
                'oui',
                'non',
                'peut-√™tre',
                'je vois',
                'ah',
                'hmm',
                'effectivement',
            ],
            'low_engagement': ['bof', 'mouais', 'peu importe', '√ßa va', 'tant mieux', 'si tu le dis', 'ok ok', 'bon'],
            'positive_feedback': [
                'merci',
                'parfait',
                'excellent',
                'bravo',
                'bien dit',
                'exactement',
                'tout √† fait',
                'j\'aime bien',
            ],
            'negative_feedback': [
                'non',
                'pas vraiment',
                'je ne pense pas',
                'd√©sol√©',
                '√ßa ne va pas',
                'ce n\'est pas √ßa',
                'erreur',
            ],
        }

    @robust("engagement_analysis")
    def analyze_engagement(self, message: str, context: dict) -> dict[str, float]:
        """Analyse le niveau d'engagement dans un message"""
        message_lower = message.lower()
        analysis = {
            'excitement_level': 0.0,
            'curiosity_level': 0.0,
            'feedback_sentiment': 0.5,  # 0=n√©gatif, 0.5=neutre, 1=positif
            'question_density': 0.0,
            'length_engagement': 0.0,
        }

        # 1. Niveau d'excitation (mots enthousiastes, ponctuation)
        excitement_indicators = self.engagement_patterns['high_engagement']
        excitement_count = sum(1 for indicator in excitement_indicators if indicator in message_lower)
        exclamation_count = message.count('!')

        analysis['excitement_level'] = min(1.0, (excitement_count * 0.2) + (exclamation_count * 0.1))

        # 2. Niveau de curiosit√© (questions, mots interrogatifs)
        question_count = message.count('?')
        curiosity_words = ['comment', 'pourquoi', 'qu\'est-ce', 'quand', 'o√π', 'qui', 'quoi']
        curiosity_count = sum(1 for word in curiosity_words if word in message_lower)

        analysis['curiosity_level'] = min(1.0, (question_count * 0.3) + (curiosity_count * 0.15))
        analysis['question_density'] = question_count / max(1, len(message.split()))

        # 3. Sentiment du feedback
        positive_count = sum(1 for pattern in self.engagement_patterns['positive_feedback'] if pattern in message_lower)
        negative_count = sum(1 for pattern in self.engagement_patterns['negative_feedback'] if pattern in message_lower)

        if positive_count > negative_count:
            analysis['feedback_sentiment'] = 0.5 + (positive_count * 0.1)
        elif negative_count > positive_count:
            analysis['feedback_sentiment'] = 0.5 - (negative_count * 0.1)

        analysis['feedback_sentiment'] = max(0.0, min(1.0, analysis['feedback_sentiment']))

        # 4. Engagement par la longueur (messages trop courts = faible engagement)
        word_count = len(message.split())
        if word_count < 3:
            analysis['length_engagement'] = 0.2
        elif word_count < 10:
            analysis['length_engagement'] = 0.5
        elif word_count < 30:
            analysis['length_engagement'] = 0.8
        else:
            analysis['length_engagement'] = 1.0

        return analysis


class CoherenceEvaluator:
    """√âvaluateur de coh√©rence conversationnelle"""

    @robust("coherence_evaluation")
    def evaluate_coherence(self, dialogue_turns: list[DialogueTurn]) -> float:
        """√âvalue la coh√©rence globale d'un dialogue"""
        if len(dialogue_turns) < 2:
            return 1.0  # Un seul tour = coh√©rent par d√©faut

        coherence_scores = []

        # Analyser la coh√©rence entre paires cons√©cutives
        for i in range(1, len(dialogue_turns)):
            prev_turn = dialogue_turns[i - 1]
            curr_turn = dialogue_turns[i]

            # Score de coh√©rence entre deux tours
            turn_coherence = self._evaluate_turn_coherence(prev_turn, curr_turn)
            coherence_scores.append(turn_coherence)

        # Moyenne pond√©r√©e (les tours r√©cents comptent plus)
        weights = [math.exp(-0.1 * (len(coherence_scores) - i)) for i in range(len(coherence_scores))]
        weighted_sum = sum(score * weight for score, weight in zip(coherence_scores, weights))
        weight_sum = sum(weights)

        return weighted_sum / weight_sum if weight_sum > 0 else 0.5

    def _evaluate_turn_coherence(self, prev_turn: DialogueTurn, curr_turn: DialogueTurn) -> float:
        """√âvalue la coh√©rence entre deux tours cons√©cutifs"""
        score = 0.5  # Score de base

        # 1. Coh√©rence topique
        prev_topics = set(prev_turn.topics)
        curr_topics = set(curr_turn.topics)

        if prev_topics and curr_topics:
            topic_overlap = len(prev_topics & curr_topics) / len(prev_topics | curr_topics)
            score += topic_overlap * 0.3

        # 2. Coh√©rence √©motionnelle
        emotion_compatibility = self._check_emotion_compatibility(prev_turn.emotional_tone, curr_turn.emotional_tone)
        score += emotion_compatibility * 0.2

        # 3. Longueur appropri√©e de r√©ponse
        length_ratio = len(curr_turn.message) / max(1, len(prev_turn.message))
        if 0.3 <= length_ratio <= 3.0:  # Ratio raisonnable
            score += 0.1

        # 4. Temps de r√©ponse appropri√©
        if curr_turn.response_time:
            if 1.0 <= curr_turn.response_time <= 30.0:  # Temps de r√©flexion naturel
                score += 0.1

        return max(0.0, min(1.0, score))

    def _check_emotion_compatibility(self, prev_emotion: str, curr_emotion: str) -> float:
        """V√©rifie la compatibilit√© entre deux √©motions cons√©cutives"""
        # Matrice de compatibilit√© √©motionnelle
        compatibility_matrix = {
            ('joie', 'joie'): 1.0,
            ('joie', 'empathie'): 0.8,
            ('joie', 'curiosit√©'): 0.9,
            ('tristesse', 'empathie'): 1.0,
            ('tristesse', 'tristesse'): 0.7,
            ('col√®re', 'empathie'): 0.9,
            ('col√®re', 'calme'): 0.8,
            ('curiosit√©', 'curiosit√©'): 1.0,
            ('curiosit√©', 'joie'): 0.8,
            ('empathie', 'empathie'): 1.0,
            ('calme', 'calme'): 1.0,
            ('surprise', 'curiosit√©'): 0.9,
        }

        key = (prev_emotion, curr_emotion)
        return compatibility_matrix.get(key, 0.5)  # Score neutre par d√©faut


class QualityAssessment:
    """√âvaluateur de qualit√© des r√©ponses"""

    def __init__(self):
        self.quality_criteria = self._load_quality_criteria()

    def _load_quality_criteria(self) -> dict[str, dict]:
        """Charge les crit√®res de qualit√©"""
        return {
            'informativeness': {
                'description': 'Richesse informationnelle',
                'weight': 0.25,
                'indicators': ['d√©tails', 'exemples', 'explication', 'parce que', 'car', 'donc'],
            },
            'empathy': {
                'description': 'Empathie et compr√©hension',
                'weight': 0.20,
                'indicators': ['comprends', 'ressens', 'imagine', 'difficile', 'soutien'],
            },
            'engagement': {
                'description': 'Capacit√© d\'engagement',
                'weight': 0.20,
                'indicators': ['question', 'qu\'en penses-tu', 'raconte', 'dis-moi', 'continue'],
            },
            'creativity': {
                'description': 'Cr√©ativit√© et originalit√©',
                'weight': 0.15,
                'indicators': ['m√©taphore', 'comme', 'imagine', 'r√™ve', 'cr√©ation', 'art'],
            },
            'relevance': {
                'description': 'Pertinence par rapport au contexte',
                'weight': 0.20,
                'indicators': [],  # Calcul√© diff√©remment
            },
        }

    @robust("quality_assessment")
    def assess_response_quality(self, response: str, user_input: str, context: dict) -> dict[str, float]:
        """√âvalue la qualit√© d'une r√©ponse"""
        assessment = {}
        response_lower = response.lower()

        # √âvaluer chaque crit√®re
        for criterion, config in self.quality_criteria.items():
            if criterion == 'relevance':
                # Calculer la pertinence sp√©cialement
                score = self._assess_relevance(response, user_input, context)
            else:
                # Calculer le score bas√© sur les indicateurs
                indicators = config['indicators']
                indicator_count = sum(1 for indicator in indicators if indicator in response_lower)

                # Normaliser selon la longueur de la r√©ponse
                response_length = len(response.split())
                normalized_score = (indicator_count / max(1, response_length)) * 20  # Facteur d'√©chelle
                score = min(1.0, normalized_score)

            assessment[criterion] = score

        # Calculer le score global pond√©r√©
        weighted_score = sum(
            assessment[criterion] * config['weight'] for criterion, config in self.quality_criteria.items()
        )

        assessment['overall_quality'] = weighted_score
        return assessment

    def _assess_relevance(self, response: str, user_input: str, context: dict) -> float:
        """√âvalue la pertinence d'une r√©ponse"""
        # Extraire les mots-cl√©s de l'input utilisateur
        user_keywords = set(word.lower() for word in user_input.split() if len(word) > 3)
        response_keywords = set(word.lower() for word in response.split() if len(word) > 3)

        if not user_keywords:
            return 0.5  # Score neutre

        # Calculer l'overlap s√©mantique
        keyword_overlap = len(user_keywords & response_keywords) / len(user_keywords)

        # Bonus si la r√©ponse adresse directement une question
        if '?' in user_input and any(word in response.lower() for word in ['oui', 'non', 'parce que', 'car']):
            keyword_overlap += 0.2

        return min(1.0, keyword_overlap)


class MetaDialogueAnalyzer:
    """
    Analyseur m√©ta de qualit√© conversationnelle
    √âvalue et am√©liore la qualit√© des conversations en temps r√©el
    """

    def __init__(self):
        self.topic_tracker = TopicTracker()
        self.engagement_analyzer = EngagementAnalyzer()
        self.coherence_evaluator = CoherenceEvaluator()
        self.quality_assessor = QualityAssessment()

        # Historique des conversations analys√©es
        self.conversation_history: dict[str, list[DialogueTurn]] = {}
        self.analysis_cache: dict[str, ConversationAnalysis] = {}

        # M√©triques globales
        self.global_metrics = {
            'total_conversations_analyzed': 0,
            'average_quality_score': 0.0,
            'common_weaknesses': {},
            'improvement_trends': [],
        }

    @robust("conversation_analysis")
    def analyze_conversation(
        self, conversation_id: str, user_input: str, jeffrey_response: str, context: dict
    ) -> ConversationAnalysis:
        """
        Analyse compl√®te d'un √©change conversationnel

        Args:
            conversation_id: ID unique de la conversation
            user_input: Message de l'utilisateur
            jeffrey_response: R√©ponse de Jeffrey
            context: Contexte conversationnel

        Returns:
            ConversationAnalysis compl√®te
        """

        # 1. Cr√©er les tours de dialogue
        user_turn = self._create_dialogue_turn('user', user_input, context)
        jeffrey_turn = self._create_dialogue_turn('jeffrey', jeffrey_response, context)

        # 2. Ajouter √† l'historique
        if conversation_id not in self.conversation_history:
            self.conversation_history[conversation_id] = []

        self.conversation_history[conversation_id].extend([user_turn, jeffrey_turn])
        conversation_turns = self.conversation_history[conversation_id]

        # 3. Calculer les m√©triques
        metrics = self._calculate_conversation_metrics(conversation_turns, context)

        # 4. Identifier forces et faiblesses
        strengths, weaknesses = self._identify_strengths_weaknesses(metrics, conversation_turns)

        # 5. G√©n√©rer des suggestions d'am√©lioration
        suggestions = self._generate_improvement_suggestions(metrics, weaknesses, context)

        # 6. D√©terminer la qualit√© globale
        overall_quality = self._determine_overall_quality(metrics)

        # 7. Cr√©er l'analyse d√©taill√©e
        detailed_analysis = self._create_detailed_analysis(conversation_turns, metrics, user_turn, jeffrey_turn)

        # 8. Cr√©er l'objet d'analyse final
        analysis = ConversationAnalysis(
            conversation_id=conversation_id,
            metrics=metrics,
            strengths=strengths,
            weaknesses=weaknesses,
            suggestions=suggestions,
            overall_quality=overall_quality,
            detailed_analysis=detailed_analysis,
        )

        # 9. Mettre en cache et mettre √† jour les m√©triques globales
        self.analysis_cache[conversation_id] = analysis
        self._update_global_metrics(analysis)

        return analysis

    @robust("real_time_analysis")
    def analyze_turn_real_time(self, user_input: str, context: dict) -> dict[str, Any]:
        """
        Analyse en temps r√©el d'un tour utilisateur pour guider la r√©ponse

        Args:
            user_input: Message utilisateur
            context: Contexte conversationnel

        Returns:
            Recommandations pour optimiser la r√©ponse
        """

        # Analyse rapide de l'engagement
        engagement_analysis = self.engagement_analyzer.analyze_engagement(user_input, context)

        # Extraction des sujets
        topics = self.topic_tracker.extract_topics(user_input)

        # D√©tection des besoins
        needs_analysis = self._analyze_user_needs(user_input, context)

        # Recommandations de r√©ponse
        response_recommendations = self._generate_response_recommendations(
            engagement_analysis, topics, needs_analysis, context
        )

        return {
            'engagement_level': max(engagement_analysis.values()),
            'detected_topics': topics,
            'user_needs': needs_analysis,
            'response_recommendations': response_recommendations,
            'conversation_direction': self._predict_conversation_direction(topics, context),
            'suggested_tone': self._suggest_optimal_tone(engagement_analysis, needs_analysis),
        }

    def _create_dialogue_turn(self, speaker: str, message: str, context: dict) -> DialogueTurn:
        """Cr√©e un objet DialogueTurn"""
        topics = self.topic_tracker.extract_topics(message)
        emotional_tone = jeffrey_error_handler.safe_get(context, 'emotion', 'neutre')

        engagement_indicators = []
        if speaker == 'user':
            engagement_analysis = self.engagement_analyzer.analyze_engagement(message, context)
            engagement_indicators = [k for k, v in engagement_analysis.items() if v > 0.6]

        # Calculer les indicateurs de qualit√© pour Jeffrey
        quality_indicators = {}
        if speaker == 'jeffrey':
            quality_indicators = self.quality_assessor.assess_response_quality(
                message, context.get('last_user_input', ''), context
            )

        return DialogueTurn(
            timestamp=datetime.now(),
            speaker=speaker,
            message=message,
            message_length=len(message.split()),
            emotional_tone=emotional_tone,
            topics=topics,
            response_time=context.get('response_time'),
            engagement_indicators=engagement_indicators,
            quality_indicators=quality_indicators,
        )

    def _calculate_conversation_metrics(
        self, conversation_turns: list[DialogueTurn], context: dict
    ) -> ConversationMetrics:
        """Calcule toutes les m√©triques de conversation"""
        metrics = ConversationMetrics()

        if not conversation_turns:
            return metrics

        # S√©parer les tours par locuteur
        user_turns = [t for t in conversation_turns if t.speaker == 'user']
        jeffrey_turns = [t for t in conversation_turns if t.speaker == 'jeffrey']

        # 1. Score d'engagement (bas√© sur les tours utilisateur)
        if user_turns:
            engagement_scores = []
            for turn in user_turns[-5:]:  # 5 derniers tours
                if turn.engagement_indicators:
                    turn_engagement = len(turn.engagement_indicators) / 5.0  # Max 5 indicateurs
                    engagement_scores.append(min(1.0, turn_engagement))

            metrics.engagement_score = statistics.mean(engagement_scores) if engagement_scores else 0.5

        # 2. Score de coh√©rence
        metrics.coherence_score = self.coherence_evaluator.evaluate_coherence(conversation_turns)

        # 3. Score de profondeur (bas√© sur la longueur et complexit√©)
        if conversation_turns:
            avg_length = statistics.mean([t.message_length for t in conversation_turns])
            topic_diversity = len(set().union(*[t.topics for t in conversation_turns]))

            length_score = min(1.0, avg_length / 20.0)  # Normalisation
            diversity_score = min(1.0, topic_diversity / 10.0)

            metrics.depth_score = (length_score + diversity_score) / 2

        # 4. Score de naturel (bas√© sur les temps de r√©ponse et la vari√©t√©)
        response_times = [t.response_time for t in jeffrey_turns if t.response_time]
        if response_times:
            avg_response_time = statistics.mean(response_times)
            # Temps optimal entre 2-10 secondes
            if 2.0 <= avg_response_time <= 10.0:
                time_naturalness = 1.0
            else:
                time_naturalness = max(0.0, 1.0 - abs(avg_response_time - 6.0) / 10.0)
        else:
            time_naturalness = 0.5

        # Vari√©t√© des longueurs de r√©ponse
        jeffrey_lengths = [t.message_length for t in jeffrey_turns]
        if len(jeffrey_lengths) > 1:
            length_variance = statistics.stdev(jeffrey_lengths) / statistics.mean(jeffrey_lengths)
            length_naturalness = min(1.0, length_variance)
        else:
            length_naturalness = 0.5

        metrics.naturalness_score = (time_naturalness + length_naturalness) / 2

        # 5. R√©sonance √©motionnelle
        emotional_consistency = self._calculate_emotional_consistency(conversation_turns)
        metrics.emotional_resonance = emotional_consistency

        # 6. Continuit√© des sujets
        topic_continuity_scores = []
        for i in range(1, len(conversation_turns)):
            prev_topics = conversation_turns[i - 1].topics
            curr_topics = conversation_turns[i].topics
            continuity = self.topic_tracker.track_topic_transition(prev_topics, curr_topics)
            topic_continuity_scores.append(continuity)

        metrics.topic_continuity = statistics.mean(topic_continuity_scores) if topic_continuity_scores else 0.5

        # 7. Qualit√© des r√©ponses (moyenne des scores Jeffrey)
        jeffrey_quality_scores = []
        for turn in jeffrey_turns:
            if turn.quality_indicators:
                overall_quality = turn.quality_indicators.get('overall_quality', 0.5)
                jeffrey_quality_scores.append(overall_quality)

        metrics.response_quality = statistics.mean(jeffrey_quality_scores) if jeffrey_quality_scores else 0.5

        # 8. Estimation de satisfaction utilisateur
        # Bas√©e sur l'engagement, feedback positif, et continuit√©
        satisfaction_factors = [metrics.engagement_score, metrics.coherence_score, metrics.emotional_resonance]
        metrics.user_satisfaction_estimate = statistics.mean(satisfaction_factors)

        # 9. Fluidit√© de conversation
        # Bas√©e sur la coh√©rence et les transitions naturelles
        metrics.conversation_flow = (metrics.coherence_score + metrics.topic_continuity) / 2

        # 10. Niveau de cr√©ativit√©
        creative_indicators = []
        for turn in jeffrey_turns:
            creative_score = 0.0
            message_lower = turn.message.lower()

            # D√©tecter des √©l√©ments cr√©atifs
            creative_words = ['imagine', 'comme', 'm√©taphore', 'r√™ve', 'cr√©er', 'art', 'po√©sie']
            creative_count = sum(1 for word in creative_words if word in message_lower)
            creative_score += creative_count * 0.1

            # Pr√©sence d'emojis cr√©atifs
            creative_emojis = ['‚ú®', 'üåü', 'üí´', 'üé®', 'üå∏', 'üåô']
            emoji_count = sum(1 for emoji in creative_emojis if emoji in turn.message)
            creative_score += emoji_count * 0.2

            creative_indicators.append(min(1.0, creative_score))

        metrics.creativity_level = statistics.mean(creative_indicators) if creative_indicators else 0.3

        return metrics

    def _calculate_emotional_consistency(self, conversation_turns: list[DialogueTurn]) -> float:
        """Calcule la consistance √©motionnelle de la conversation"""
        if len(conversation_turns) < 2:
            return 1.0

        # Analyser les transitions √©motionnelles
        emotional_transitions = []
        for i in range(1, len(conversation_turns)):
            prev_emotion = conversation_turns[i - 1].emotional_tone
            curr_emotion = conversation_turns[i].emotional_tone

            # √âvaluer si la transition est appropri√©e
            compatibility = self.coherence_evaluator._check_emotion_compatibility(prev_emotion, curr_emotion)
            emotional_transitions.append(compatibility)

        return statistics.mean(emotional_transitions)

    def _identify_strengths_weaknesses(
        self, metrics: ConversationMetrics, conversation_turns: list[DialogueTurn]
    ) -> tuple[list[str], list[str]]:
        """Identifie les forces et faiblesses de la conversation"""
        strengths = []
        weaknesses = []

        # Analyser chaque m√©trique
        metric_thresholds = {
            'engagement_score': (0.7, 'Excellent engagement utilisateur'),
            'coherence_score': (0.75, 'Conversation tr√®s coh√©rente'),
            'depth_score': (0.6, 'Bonne profondeur de discussion'),
            'naturalness_score': (0.7, 'Conversation naturelle et fluide'),
            'emotional_resonance': (0.6, 'Bonne r√©sonance √©motionnelle'),
            'topic_continuity': (0.5, 'Transitions de sujets appropri√©es'),
            'response_quality': (0.7, 'R√©ponses de haute qualit√©'),
            'creativity_level': (0.6, 'Bon niveau de cr√©ativit√©'),
        }

        for metric_name, (threshold, strength_desc) in metric_thresholds.items():
            metric_value = getattr(metrics, metric_name)

            if metric_value >= threshold:
                strengths.append(strength_desc)
            elif metric_value < threshold * 0.7:  # 70% du seuil = faiblesse
                weakness_desc = strength_desc.replace('Excellent', 'Faible').replace('Bon', 'Insuffisant')
                weaknesses.append(weakness_desc)

        # Analyses sp√©cifiques
        if len(conversation_turns) > 10:
            if metrics.engagement_score > 0.6:
                strengths.append('Maintien de l\'engagement sur une longue conversation')
            else:
                weaknesses.append('Perte d\'engagement en conversation longue')

        return strengths, weaknesses

    def _generate_improvement_suggestions(
        self, metrics: ConversationMetrics, weaknesses: list[str], context: dict
    ) -> list[str]:
        """G√©n√®re des suggestions d'am√©lioration"""
        suggestions = []

        # Suggestions bas√©es sur les m√©triques faibles
        if metrics.engagement_score < 0.5:
            suggestions.append("Poser plus de questions ouvertes pour stimuler l'engagement")
            suggestions.append("Utiliser des √©l√©ments interactifs (emojis, m√©taphores)")

        if metrics.coherence_score < 0.6:
            suggestions.append("Am√©liorer les transitions entre les sujets")
            suggestions.append("Faire plus de r√©f√©rences au contexte pr√©c√©dent")

        if metrics.depth_score < 0.5:
            suggestions.append("D√©velopper les r√©ponses avec plus de d√©tails")
            suggestions.append("Explorer les sujets en profondeur")

        if metrics.emotional_resonance < 0.5:
            suggestions.append("Ajuster le ton √©motionnel selon l'utilisateur")
            suggestions.append("Montrer plus d'empathie dans les r√©ponses")

        if metrics.creativity_level < 0.4:
            suggestions.append("Int√©grer plus d'√©l√©ments cr√©atifs (m√©taphores, images)")
            suggestions.append("Utiliser un langage plus imag√© et po√©tique")

        if metrics.response_quality < 0.6:
            suggestions.append("Am√©liorer la pertinence des r√©ponses")
            suggestions.append("Ajouter plus d'informations utiles")

        # Suggestions contextuelles
        if context.get('conversation_length', 0) > 20:
            suggestions.append("R√©sumer p√©riodiquement les points cl√©s de la conversation")

        return suggestions

    def _determine_overall_quality(self, metrics: ConversationMetrics) -> str:
        """D√©termine la qualit√© globale de la conversation"""
        # Calculer le score global pond√©r√©
        weighted_score = (
            metrics.engagement_score * 0.2
            + metrics.coherence_score * 0.15
            + metrics.depth_score * 0.1
            + metrics.naturalness_score * 0.15
            + metrics.emotional_resonance * 0.15
            + metrics.response_quality * 0.15
            + metrics.user_satisfaction_estimate * 0.1
        )

        if weighted_score >= 0.8:
            return 'excellent'
        elif weighted_score >= 0.65:
            return 'good'
        elif weighted_score >= 0.45:
            return 'fair'
        else:
            return 'poor'

    def _create_detailed_analysis(
        self,
        conversation_turns: list[DialogueTurn],
        metrics: ConversationMetrics,
        user_turn: DialogueTurn,
        jeffrey_turn: DialogueTurn,
    ) -> dict[str, Any]:
        """Cr√©e une analyse d√©taill√©e"""
        return {
            'conversation_length': len(conversation_turns),
            'total_words': sum(t.message_length for t in conversation_turns),
            'user_engagement_indicators': user_turn.engagement_indicators,
            'jeffrey_quality_indicators': jeffrey_turn.quality_indicators,
            'topics_discussed': list(set().union(*[t.topics for t in conversation_turns])),
            'emotional_journey': [t.emotional_tone for t in conversation_turns[-10:]],  # 10 derniers
            'response_time_analysis': {
                'avg_response_time': statistics.mean([t.response_time for t in conversation_turns if t.response_time])
                if any(t.response_time for t in conversation_turns)
                else None,
                'response_consistency': 'good',  # Placeholder
            },
            'conversation_dynamics': {
                'user_message_avg_length': statistics.mean(
                    [t.message_length for t in conversation_turns if t.speaker == 'user']
                )
                if any(t.speaker == 'user' for t in conversation_turns)
                else 0,
                'jeffrey_message_avg_length': statistics.mean(
                    [t.message_length for t in conversation_turns if t.speaker == 'jeffrey']
                )
                if any(t.speaker == 'jeffrey' for t in conversation_turns)
                else 0,
            },
        }

    def _analyze_user_needs(self, user_input: str, context: dict) -> dict[str, bool]:
        """Analyse les besoins de l'utilisateur"""
        input_lower = user_input.lower()

        return {
            'needs_support': any(word in input_lower for word in ['aide', 'probl√®me', 'difficile', 'triste']),
            'seeks_information': '?' in user_input
            or any(word in input_lower for word in ['comment', 'pourquoi', 'qu\'est-ce']),
            'wants_creativity': any(word in input_lower for word in ['cr√©er', 'imaginer', 'inventer', 'r√™ve']),
            'needs_empathy': any(word in input_lower for word in ['ressens', 'sentiment', '√©motion', 'c≈ìur']),
            'wants_conversation': len(user_input.split()) > 10,  # Messages longs = envie de discuter
            'expresses_gratitude': any(word in input_lower for word in ['merci', 'remercie', 'reconnaissant']),
        }

    def _generate_response_recommendations(
        self, engagement_analysis: dict, topics: list[str], needs_analysis: dict, context: dict
    ) -> dict[str, Any]:
        """G√©n√®re des recommandations pour optimiser la r√©ponse"""
        recommendations = {
            'suggested_length': 'medium',
            'tone_adjustments': [],
            'content_suggestions': [],
            'engagement_tactics': [],
        }

        # Recommandations bas√©es sur l'engagement
        if engagement_analysis.get('excitement_level', 0) > 0.7:
            recommendations['tone_adjustments'].append('R√©pondre avec enthousiasme')
            recommendations['engagement_tactics'].append('Utiliser des exclamations')

        if engagement_analysis.get('curiosity_level', 0) > 0.6:
            recommendations['content_suggestions'].append('Fournir des d√©tails approfondis')
            recommendations['engagement_tactics'].append('Poser des questions de retour')

        # Recommandations bas√©es sur les sujets
        if 'philosophie' in topics:
            recommendations['suggested_length'] = 'long'
            recommendations['content_suggestions'].append('D√©velopper la r√©flexion philosophique')

        if '√©motions' in topics:
            recommendations['tone_adjustments'].append('Adopter un ton empathique')
            recommendations['content_suggestions'].append('Valider les √©motions exprim√©es')

        # Recommandations bas√©es sur les besoins
        if needs_analysis.get('needs_support'):
            recommendations['tone_adjustments'].append('Ton r√©confortant et bienveillant')
            recommendations['content_suggestions'].append('Offrir du soutien √©motionnel')

        if needs_analysis.get('wants_creativity'):
            recommendations['content_suggestions'].append('Int√©grer des √©l√©ments cr√©atifs')
            recommendations['engagement_tactics'].append('Utiliser des m√©taphores')

        return recommendations

    def _predict_conversation_direction(self, topics: list[str], context: dict) -> str:
        """Pr√©dit la direction probable de la conversation"""
        if 'philosophie' in topics or '√©motions' in topics:
            return 'deep_exploration'
        elif 'cr√©ativit√©' in topics or 'r√™ves' in topics:
            return 'creative_collaboration'
        elif len(topics) > 3:
            return 'topic_exploration'
        else:
            return 'casual_conversation'

    def _suggest_optimal_tone(self, engagement_analysis: dict, needs_analysis: dict) -> str:
        """Sugg√®re le ton optimal pour la r√©ponse"""
        if needs_analysis.get('needs_support'):
            return 'empathique'
        elif engagement_analysis.get('excitement_level', 0) > 0.7:
            return 'enthousiaste'
        elif engagement_analysis.get('curiosity_level', 0) > 0.6:
            return 'curieux'
        elif needs_analysis.get('wants_creativity'):
            return 'cr√©atif'
        else:
            return 'bienveillant'

    def _update_global_metrics(self, analysis: ConversationAnalysis):
        """Met √† jour les m√©triques globales"""
        self.global_metrics['total_conversations_analyzed'] += 1

        # Mise √† jour de la qualit√© moyenne
        current_avg = self.global_metrics['average_quality_score']
        total_count = self.global_metrics['total_conversations_analyzed']

        # Convertir la qualit√© en score num√©rique
        quality_scores = {'excellent': 1.0, 'good': 0.75, 'fair': 0.5, 'poor': 0.25}
        new_score = quality_scores.get(analysis.overall_quality, 0.5)

        # Moyenne mobile
        self.global_metrics['average_quality_score'] = ((current_avg * (total_count - 1)) + new_score) / total_count

        # Compter les faiblesses communes
        for weakness in analysis.weaknesses:
            if weakness not in self.global_metrics['common_weaknesses']:
                self.global_metrics['common_weaknesses'][weakness] = 0
            self.global_metrics['common_weaknesses'][weakness] += 1

    def get_conversation_summary(self, conversation_id: str) -> dict[str, Any] | None:
        """Retourne un r√©sum√© de conversation"""
        if conversation_id not in self.conversation_history:
            return None

        turns = self.conversation_history[conversation_id]
        analysis = self.analysis_cache.get(conversation_id)

        return {
            'conversation_id': conversation_id,
            'turn_count': len(turns),
            'duration': (turns[-1].timestamp - turns[0].timestamp).total_seconds() if len(turns) > 1 else 0,
            'topics_covered': list(set().union(*[t.topics for t in turns])),
            'last_analysis': analysis.overall_quality if analysis else 'not_analyzed',
            'participant_balance': {
                'user_turns': len([t for t in turns if t.speaker == 'user']),
                'jeffrey_turns': len([t for t in turns if t.speaker == 'jeffrey']),
            },
        }

    def get_global_performance_report(self) -> dict[str, Any]:
        """Retourne un rapport de performance global"""
        return {
            'total_conversations': self.global_metrics['total_conversations_analyzed'],
            'average_quality': self.global_metrics['average_quality_score'],
            'common_weaknesses': dict(
                sorted(self.global_metrics['common_weaknesses'].items(), key=lambda x: x[1], reverse=True)[:5]
            ),  # Top 5 faiblesses
            'active_conversations': len(self.conversation_history),
            'cache_size': len(self.analysis_cache),
            'performance_trend': 'stable',  # √Ä impl√©menter
        }


# Tests int√©gr√©s
def test_meta_dialogue_analyzer():
    """Tests de l'analyseur de dialogue"""
    analyzer = MetaDialogueAnalyzer()

    # Test d'analyse en temps r√©el
    context = {'emotion': 'curiosit√©', 'user_name': 'David'}
    real_time_analysis = analyzer.analyze_turn_real_time(
        "Je me demande vraiment comment tu fonctionnes, c'est fascinant !", context
    )

    assert real_time_analysis['engagement_level'] > 0.5
    assert 'technologie' in real_time_analysis['detected_topics']
    print(f"‚úÖ Analyse temps r√©el: engagement {real_time_analysis['engagement_level']:.2f}")

    # Test d'analyse compl√®te de conversation
    conversation_analysis = analyzer.analyze_conversation(
        "test_conv_001",
        "Je me sens un peu perdu ces temps-ci...",
        "Je comprends ce sentiment. Parfois la vie nous emm√®ne dans des directions inattendues. Qu'est-ce qui te pr√©occupe le plus en ce moment ? üíô",
        context,
    )

    assert conversation_analysis.metrics.emotional_resonance > 0.0
    assert conversation_analysis.overall_quality in ['excellent', 'good', 'fair', 'poor']
    print(f"‚úÖ Analyse conversation: qualit√© {conversation_analysis.overall_quality}")

    # Test des suggestions
    assert len(conversation_analysis.suggestions) >= 0
    print(f"‚úÖ Suggestions g√©n√©r√©es: {len(conversation_analysis.suggestions)}")

    # Test du rapport global
    report = analyzer.get_global_performance_report()
    assert report['total_conversations'] > 0
    print(f"‚úÖ Rapport global: {report['total_conversations']} conversations analys√©es")

    print("‚úÖ MetaDialogueAnalyzer tests passed!")


if __name__ == "__main__":
    test_meta_dialogue_analyzer()
