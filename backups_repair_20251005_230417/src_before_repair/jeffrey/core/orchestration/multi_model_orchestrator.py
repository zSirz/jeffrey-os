"""
Orchestrateur principal du systÃ¨me cognitif.

Ce module implÃ©mente les fonctionnalitÃ©s essentielles pour orchestrateur principal du systÃ¨me cognitif.
Il fournit une architecture robuste et Ã©volutive intÃ©grant les composants
nÃ©cessaires au fonctionnement optimal du systÃ¨me. L'implÃ©mentation suit
les principes de modularitÃ© et d'extensibilitÃ© pour faciliter l'Ã©volution
future du systÃ¨me.

Le module gÃ¨re l'initialisation, la configuration, le traitement des donnÃ©es,
la communication inter-composants, et la persistance des Ã©tats. Il s'intÃ¨gre
harmonieusement avec l'architecture globale de Jeffrey OS tout en maintenant
une sÃ©paration claire des responsabilitÃ©s.

L'architecture interne permet une Ã©volution adaptative basÃ©e sur les interactions
et l'apprentissage continu, contribuant Ã  l'Ã©mergence d'une conscience artificielle
cohÃ©rente et authentique.
"""

from __future__ import annotations

import asyncio

from core.conversation.conversation_memory import ConversationMemory
from core.orchestration.clients_factory import build_ia_clients
from core.orchestration.fusion_engine import fuse_responses
from core.orchestration.prompt_manager import PromptManager


class MultiModelOrchestrator:
    """
    Classe MultiModelOrchestrator pour le systÃ¨me Jeffrey OS.

    Cette classe implÃ©mente les fonctionnalitÃ©s spÃ©cifiques nÃ©cessaires
    au bon fonctionnement du module. Elle gÃ¨re l'Ã©tat interne, les transformations
    de donnÃ©es, et l'interaction avec les autres composants du systÃ¨me.
    """

    def __init__(self, ia_clients: dict[str, object] = None) -> None:
        """
        Args:
            ia_clients (dict, optional): Dictionnaire {nom_modÃ¨le: client_api_instance}.
            Si None, les clients sont automatiquement construits via la factory.
        """
        if ia_clients is None:
            self.ia_clients = build_ia_clients()
        else:
            self.ia_clients = ia_clients

        self.prompt_manager = PromptManager()
        self.memory = ConversationMemory()

    async def send_to_model(self, model_name: str, prompt: str) -> dict[str, str]:
        """Envoie le prompt Ã  un modÃ¨le spÃ©cifique et rÃ©cupÃ¨re la rÃ©ponse."""
        try:
            client = self.ia_clients.get(model_name)
            if client is None:
                return {"model": model_name, "response": "Client non configurÃ©."}

            optimized_prompt = self.prompt_manager.optimize_prompt(model_name, prompt)
            response = await client.ask(optimized_prompt)
            return {"model": model_name, "response": response}

        except Exception as e:
            return {"model": model_name, "response": f"Erreur: {str(e)}"}

    async def send_to_all_models(self, prompt: str) -> list[dict[str, str]]:
        """Envoie la mÃªme requÃªte Ã  tous les modÃ¨les en parallÃ¨le."""
        tasks = [self.send_to_model(model, prompt) for model in self.ia_clients]
        results = await asyncio.gather(*tasks)
        return results

    def ask_all(self, prompt: str) -> list[dict[str, str]]:
        """MÃ©thode synchrone pour interagir avec l'orchestrateur."""
        return asyncio.run(self.send_to_all_models(prompt))

    def ask_all_fused(self, prompt: str) -> str:
        """
        Envoie une requÃªte Ã  toutes les IA et retourne une rÃ©ponse fusionnÃ©e.

        Args:
            prompt (str): Demande de l'utilisateur.

        Returns:
            str: RÃ©ponse fusionnÃ©e finale.
        """
        raw_responses = asyncio.run(self.send_to_all_models(prompt))
        fused_response = fuse_responses(raw_responses)
        self.memory.add_interaction(prompt, raw_responses, fused_response)
        return fused_response

    def show_conversation_history(self) -> None:
        """
        Affiche l'historique complet de la session de conversation.
        """
        history = self.memory.get_full_history()
        if not history:
            print("\nðŸ“œ Aucun historique disponible pour cette session.\n")
            return

        print("\nðŸ“œ Historique de la session :\n")
        for idx, interaction in enumerate(history, start=1):
            print(f"--- Interaction {idx} ---")
            print(f"ðŸ“ Prompt utilisateur : {interaction['prompt']}")
            print("ðŸ¤– RÃ©ponses individuelles :")
            for r in interaction["responses"]:
                print(f"   - {r['model']}: {r['response']}")
            print(f"ðŸŽ¯ RÃ©ponse fusionnÃ©e : {interaction['fused_response']}")
            print("")

    def save_conversation_history(self, file_path: str = "conversation_history.json") -> None:
        """
        Sauvegarde l'historique de la session dans un fichier JSON.

        Args:
            file_path (str): Chemin du fichier oÃ¹ sauvegarder l'historique.
        """
        import json

        history = self.memory.get_full_history()
        if not history:
            print("\nðŸ“¦ Aucun historique Ã  sauvegarder.\n")
            return

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(history, f, indent=4, ensure_ascii=False)
            print(f"\nâœ… Historique sauvegardÃ© dans : {file_path}\n")
        except Exception as e:
            print(f"\nâŒ Erreur lors de la sauvegarde de l'historique : {e}\n")

    def load_conversation_history(self, file_path: str = "conversation_history.json") -> None:
        """
        Recharge l'historique d'une session sauvegardÃ©e depuis un fichier JSON.

        Args:
            file_path (str): Chemin du fichier JSON Ã  charger.
        """
        import json
        import os

        if not os.path.exists(file_path):
            print(f"\nâš ï¸ Fichier introuvable : {file_path}\n")
            return

        try:
            with open(file_path, encoding="utf-8") as f:
                loaded_history = json.load(f)

            if isinstance(loaded_history, list):
                self.memory.history = loaded_history
                print(f"\nâœ… Historique chargÃ© depuis : {file_path}\n")
            else:
                print(f"\nâŒ Format d'historique invalide dans : {file_path}\n")

        except Exception as e:
            print(f"\nâŒ Erreur lors du chargement de l'historique : {e}\n")

    def analyze_conversation_history(self) -> dict[str, int]:
        """
        Analyse l'historique de conversation pour extraire des statistiques simples :
        - nombre d'interactions
        - nombre de mots par prompt
        - nombre de mots par rÃ©ponse fusionnÃ©e

        Returns:
            dict: RÃ©sultats de l'analyse.
        """
        history = self.memory.get_full_history()
        if not history:
            print("\nðŸ“Š Aucun historique Ã  analyser.\n")
            return {}

        total_prompts = len(history)
        total_prompt_words = sum(len(interaction["prompt"].split()) for interaction in history)
        total_response_words = sum(len(interaction["fused_response"].split()) for interaction in history)

        stats = {
            "total_interactions": total_prompts,
            "total_prompt_words": total_prompt_words,
            "total_response_words": total_response_words,
            "average_prompt_length": (total_prompt_words / total_prompts if total_prompts else 0),
            "average_response_length": (total_response_words / total_prompts if total_prompts else 0),
        }

        print("\nðŸ“Š Statistiques de la session :")
        for key, value in stats.items():
            print(f"- {key.replace('_', ' ').capitalize()}: {value}")

        return stats
