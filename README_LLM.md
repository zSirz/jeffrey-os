# ğŸ§  Jeffrey OS - Guide LLM (Ollama/vLLM)

## ğŸš€ Quick Start pour Mac

```bash
# 1. Installer Ollama et tÃ©lÃ©charger les modÃ¨les
./scripts/install_ollama.sh

# 2. DÃ©marrer le serveur LLM adaptatif
./scripts/start_llm_server.sh

# 3. Tester que tout fonctionne
python tests/test_llm_adaptive.py

# 4. Lancer Jeffrey OS
python main.py
```

## ğŸ“¦ Architecture Dual-Mode

Jeffrey OS dÃ©tecte automatiquement votre environnement :

| Environnement | Backend | ModÃ¨le | Performance |
|---------------|---------|--------|-------------|
| **macOS** | Ollama | Mistral 7B | âš¡ OptimisÃ© Apple Silicon |
| **Linux+GPU** | vLLM | Apertus 8B | ğŸ® Max performance GPU |
| **Linux CPU** | Ollama | Mistral 7B | ğŸ’» Mode CPU efficace |

## ğŸ”§ Configuration

Le fichier `.env.p2` contient :
```env
LLM_MODEL_OLLAMA=mistral:7b-instruct  # Pour Mac/CPU
LLM_MODEL_VLLM=swiss-ai/Apertus-8B    # Pour GPU
```

## ğŸ§ª Tests

```bash
# Test adaptatif (dÃ©tecte l'environnement)
python tests/test_llm_adaptive.py

# Tests complets avec pytest
pytest tests/test_llm_adaptive.py -v

# Test de charge
python tests/test_apertus_load.py
```

## ğŸ“Š Monitoring

```bash
# Dashboard temps rÃ©el
python scripts/monitor_apertus.py

# MÃ©triques Prometheus
curl http://localhost:9010/metrics
```

## ğŸŒ‰ API OpenAI Compatible

Les deux backends exposent la mÃªme API :
- Endpoint : `http://localhost:9010/v1`
- ModÃ¨les : `/v1/models`
- Chat : `/v1/chat/completions`
- MÃ©triques : `/metrics`

## ğŸ™ ModÃ¨les Ollama disponibles

```bash
# RecommandÃ© (installÃ© par dÃ©faut)
ollama pull mistral:7b-instruct

# Alternatives
ollama pull llama3.2:3b      # Plus lÃ©ger
ollama pull phi3:medium       # Ultra-rapide
ollama pull codellama:13b     # Pour le code
```

## â˜ï¸ Options Cloud pour Apertus

Si vous voulez utiliser le vrai modÃ¨le Apertus :

### RunPod ($0.39/h)
```bash
# Sur RunPod
vllm serve swiss-ai/Apertus-8B-Instruct-2509 --port 8000

# Sur votre Mac
LLM_BASE_URL=https://<pod-id>-8000.proxy.runpod.net/v1
```

### Google Colab (Gratuit)
```python
!pip install vllm
!vllm serve swiss-ai/Apertus-8B-Instruct-2509 --share
```

## ğŸ” Troubleshooting

### Ollama ne dÃ©marre pas sur Mac
```bash
# Installer via Homebrew
brew install ollama

# Ou rÃ©installer
curl -fsSL https://ollama.com/install.sh | sh
```

### Port 9010 occupÃ©
```bash
# Changer le port dans .env.p2
LLM_PORT=9011

# Relancer
./scripts/start_llm_server.sh
```

### ModÃ¨le trop lent
```bash
# Utiliser un modÃ¨le plus lÃ©ger
ollama pull phi3:mini
export LLM_MODEL_OLLAMA=phi3:mini
```

## ğŸ“ˆ Performance Tips

1. **Sur Mac M1/M2** : Ollama utilise automatiquement Metal
2. **RAM minimum** : 8GB pour Mistral 7B
3. **Cache** : Les modÃ¨les sont cachÃ©s dans `~/.ollama/models`
4. **Concurrence** : Le proxy supporte plusieurs requÃªtes simultanÃ©es

## ğŸ¯ Prochaines Ã©tapes

1. âœ… Installation complÃ¨te
2. âœ… Tests passent
3. ğŸš€ Jeffrey OS avec LLM local
4. ğŸ“Š Monitor les performances
5. ğŸ”§ Ajuster les paramÃ¨tres selon vos besoins
