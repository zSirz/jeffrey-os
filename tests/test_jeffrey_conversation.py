"""
Test de conversation compl√®te avec Jeffrey
V√©rifie que le syst√®me peut g√©n√©rer de vraies r√©ponses via Ollama
"""

import asyncio
import os
import sys
import time

# Setup paths
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.jeffrey.core.response.basal_ganglia_ucb1 import BasalGangliaScheduler
from src.jeffrey.core.response.neural_blackboard_v2 import NeuralBlackboard
from src.jeffrey.core.response.neural_response_orchestrator import NeuralResponseOrchestrator

# Chercher si IntelligentResponseGenerator existe
try:
    from src.jeffrey.core.consciousness.real_intelligence import IntelligentResponseGenerator

    print("‚úÖ IntelligentResponseGenerator trouv√©")
except ImportError:
    print("‚ö†Ô∏è IntelligentResponseGenerator non trouv√©, utilisation du syst√®me par d√©faut")


class ConversationContext:
    """Contexte pour simuler une conversation"""

    def __init__(self, text: str, user_id: str = "test_user"):
        self.correlation_id = f"{user_id}_{int(time.time() * 1000)}"
        self.user_input = text
        self.intent = "conversation"
        self.user_id = user_id
        self.signal = type("Signal", (), {"start_time": time.time(), "deadline_absolute": time.time() + 30})()

    def remaining_budget_ms(self):
        return 10000  # 10 secondes de budget


async def test_single_response():
    """Test d'une r√©ponse unique"""
    print("\n" + "=" * 60)
    print("üß™ TEST 1: R√âPONSE UNIQUE")
    print("=" * 60)

    # Initialiser le syst√®me
    blackboard = NeuralBlackboard(max_memory_mb=64)
    scheduler = BasalGangliaScheduler()
    orchestrator = NeuralResponseOrchestrator(None, blackboard, scheduler)

    # Question de test
    question = "Bonjour Jeffrey, pr√©sente-toi en quelques mots."
    print(f"\nüë§ User: {question}")

    context = ConversationContext(question)
    start = time.time()

    try:
        result = await orchestrator.process(context)
        elapsed = (time.time() - start) * 1000

        # R√©cup√©rer la r√©ponse de diff√©rents endroits possibles
        response = None

        # Chercher dans diff√©rents attributs
        if hasattr(result, "final_response"):
            response = result.final_response
        elif hasattr(result, "final_text"):
            response = result.final_text
        elif hasattr(result, "text"):
            response = result.text
        elif hasattr(result, "response"):
            response = result.response

        # Si pas de r√©ponse directe, chercher dans les phases
        if not response and hasattr(result, "phase_results"):
            for phase, phase_result in result.phase_results.items():
                if hasattr(phase_result, "results"):
                    for module_id, module_result in phase_result.results.items():
                        if isinstance(module_result, dict) and "response" in module_result:
                            response = module_result["response"]
                            break
                if response:
                    break

        if response:
            print(f"\nü§ñ Jeffrey: {response}")
            print(f"\n‚è±Ô∏è Temps de r√©ponse: {elapsed:.1f}ms")
            print("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s!")
        else:
            print("\n‚ùå Pas de r√©ponse g√©n√©r√©e")
            print(f"   Result type: {type(result)}")
            print(f"   Result attributes: {dir(result)}")
            if hasattr(result, "__dict__"):
                print(f"   Result content: {result.__dict__}")

    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        import traceback

        traceback.print_exc()


async def test_conversation_flow():
    """Test d'une conversation multi-tours"""
    print("\n" + "=" * 60)
    print("üß™ TEST 2: CONVERSATION MULTI-TOURS")
    print("=" * 60)

    blackboard = NeuralBlackboard(max_memory_mb=64)
    scheduler = BasalGangliaScheduler()
    orchestrator = NeuralResponseOrchestrator(None, blackboard, scheduler)

    # Sc√©nario de conversation
    conversation = [
        "Bonjour Jeffrey, je m'appelle David",
        "Je suis d√©veloppeur et je travaille sur des projets d'IA",
        "Qu'est-ce que tu peux faire pour m'aider dans mes projets ?",
        "Peux-tu me donner un exemple concret ?",
        "Merci, c'√©tait tr√®s utile !",
    ]

    user_id = "david_test"
    conversation_history = []

    for i, message in enumerate(conversation, 1):
        print(f"\n--- Tour {i} ---")
        print(f"üë§ David: {message}")

        context = ConversationContext(message, user_id)

        try:
            start = time.time()
            result = await orchestrator.process(context)
            elapsed = (time.time() - start) * 1000

            # Extraire la r√©ponse
            response = getattr(result, "final_response", None) or getattr(result, "text", None) or "..."

            print(f"ü§ñ Jeffrey: {response[:300]}{'...' if len(response) > 300 else ''}")
            print(f"‚è±Ô∏è {elapsed:.1f}ms")

            # Ajouter √† l'historique
            conversation_history.append({"user": message, "assistant": response, "latency_ms": elapsed})

            # Petite pause entre les tours
            await asyncio.sleep(0.5)

        except Exception as e:
            print(f"‚ùå Erreur tour {i}: {e}")

    # R√©sum√© de la conversation
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DE LA CONVERSATION")
    print("=" * 60)
    print(f"Tours compl√©t√©s: {len(conversation_history)}/{len(conversation)}")
    if conversation_history:
        avg_latency = sum(h["latency_ms"] for h in conversation_history) / len(conversation_history)
        print(f"Latence moyenne: {avg_latency:.1f}ms")


async def test_different_intents():
    """Test avec diff√©rents types d'intentions"""
    print("\n" + "=" * 60)
    print("üß™ TEST 3: DIFF√âRENTES INTENTIONS")
    print("=" * 60)

    blackboard = NeuralBlackboard(max_memory_mb=64)
    scheduler = BasalGangliaScheduler()
    orchestrator = NeuralResponseOrchestrator(None, blackboard, scheduler)

    # Diff√©rents types de questions
    test_cases = [
        ("Quelle est la capitale de la France ?", "question"),
        ("Je me sens un peu triste aujourd'hui", "emotion"),
        ("Raconte-moi une blague", "entertainment"),
        ("Comment fonctionne un r√©seau de neurones ?", "technical"),
        ("Au revoir Jeffrey !", "farewell"),
    ]

    results = []

    for question, intent_type in test_cases:
        print(f"\n[{intent_type.upper()}]")
        print(f"üë§ User: {question}")

        context = ConversationContext(question)
        context.intent = intent_type

        try:
            start = time.time()
            result = await orchestrator.process(context)
            elapsed = (time.time() - start) * 1000

            response = getattr(result, "final_response", None) or getattr(result, "text", None) or "Pas de r√©ponse"

            print(f"ü§ñ Jeffrey: {response[:200]}{'...' if len(response) > 200 else ''}")

            results.append({"intent": intent_type, "success": len(response) > 0, "latency_ms": elapsed})

        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            results.append({"intent": intent_type, "success": False, "latency_ms": 0})

    # R√©sum√©
    print("\nüìä R√©sultats par intention:")
    for r in results:
        status = "‚úÖ" if r["success"] else "‚ùå"
        print(f"  {status} {r['intent']}: {r['latency_ms']:.1f}ms")


async def check_ollama_connection():
    """V√©rifie la connexion √† Ollama"""
    print("\nüîç V√©rification de la connexion Ollama...")

    import aiohttp

    # Essayer diff√©rents ports o√π Ollama pourrait √™tre
    ports = [11434, 9010, 8080]

    for port in ports:
        try:
            async with aiohttp.ClientSession() as session:
                # Essayer l'endpoint standard Ollama
                async with session.get(
                    f"http://localhost:{port}/api/tags", timeout=aiohttp.ClientTimeout(total=2)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        models = data.get("models", [])
                        print(f"‚úÖ Ollama trouv√© sur port {port}")
                        if models:
                            print("   Mod√®les disponibles:")
                            for model in models:
                                size = model.get("size", 0) / (1024**3)  # Convert to GB
                                print(f"     - {model['name']} ({size:.1f}GB)")
                        return True
        except:
            continue

    print("‚ùå Ollama non accessible")
    print("   Lancez: ollama serve")
    return False


async def main():
    """Lance tous les tests"""
    print("\n" + "=" * 70)
    print("üöÄ TEST DE CONVERSATION AVEC JEFFREY")
    print("=" * 70)

    # V√©rifier Ollama d'abord
    if not await check_ollama_connection():
        print("\n‚ö†Ô∏è Ollama n'est pas accessible. Les r√©ponses seront limit√©es.")
        print("Pour de meilleures r√©ponses, lancez: ollama serve")
        response = input("\nContinuer quand m√™me ? (o/n): ")
        if response.lower() != "o":
            return

    # Lancer les tests
    try:
        await test_single_response()
        await asyncio.sleep(1)

        await test_conversation_flow()
        await asyncio.sleep(1)

        await test_different_intents()

        print("\n" + "=" * 70)
        print("‚úÖ TESTS TERMIN√âS")
        print("=" * 70)

        print("\nüìå Prochaines √©tapes:")
        print("1. Si les r√©ponses sont vides ‚Üí v√©rifier la connexion Ollama")
        print("2. Si les r√©ponses sont g√©n√©riques ‚Üí v√©rifier le mod√®le Mistral")
        print("3. Si tout fonctionne ‚Üí cr√©er l'interface web!")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è Tests interrompus")
    except Exception as e:
        print(f"\n‚ùå Erreur inattendue: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    # Afficher les informations syst√®me
    print("\nüìã Informations syst√®me:")
    print(f"   Python: {sys.version}")
    print(f"   R√©pertoire: {os.getcwd()}")
    print(f"   PYTHONPATH: {sys.path[0]}")

    # Lancer les tests
    asyncio.run(main())
