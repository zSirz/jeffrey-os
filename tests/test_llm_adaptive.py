"""
Test adaptatif qui fonctionne avec vLLM ou Ollama
"""

import asyncio
import platform

import pytest

from jeffrey.core.llm.apertus_client import ApertusClient


async def test_adaptive():
    """Test qui s'adapte Ã  l'environnement"""

    print(f"ğŸ” Platform: {platform.system()}")
    print(f"ğŸ” Architecture: {platform.machine()}")

    client = ApertusClient()
    print(f"ğŸ“¦ Using backend: {client.backend_type}")
    print(f"ğŸ“¦ Using model: {client.config.model}")

    # Test simple
    response, metadata = await client.chat(
        "Tu es Jeffrey, un assistant IA symbiotique crÃ©Ã© en Suisse.",
        "DÃ©cris-toi en une phrase en franÃ§ais.",
    )

    print(f"\nğŸ¤– Response: {response}")
    print(f"âš¡ Latency: {metadata['latency_ms']:.2f}ms")
    print(f"ğŸ“Š Model used: {metadata['model']}")

    # Test de cohÃ©rence
    assert len(response) > 10
    assert metadata["latency_ms"] < 30000  # 30 secondes max (pour la premiÃ¨re requÃªte Ollama)

    print("\nâœ… All tests passed!")


@pytest.mark.asyncio
async def test_adaptive_pytest():
    """Version pytest du test adaptatif"""
    client = ApertusClient()

    response, metadata = await client.chat(
        "Tu es Jeffrey, un assistant IA symbiotique.", "RÃ©ponds simplement 'OK' si tu comprends."
    )

    assert response is not None
    assert len(response) > 0
    assert metadata["latency_ms"] > 0
    assert client.backend_type in ["ollama", "vllm"]


if __name__ == "__main__":
    asyncio.run(test_adaptive())
