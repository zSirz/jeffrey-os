#!/usr/bin/env python3
"""
Test direct de la connexion Jeffrey ‚Üí Ollama
V√©rifie que Jeffrey g√©n√®re de vraies r√©ponses (pas des stubs)
"""

import asyncio
import os
import sys
import time

# Setup paths
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import ApertusClient qui g√®re la connexion Ollama
from src.jeffrey.core.llm.apertus_client import ApertusClient


def check_if_stub(response: str) -> bool:
    """V√©rifie si la r√©ponse est un stub/placeholder"""
    stub_patterns = [
        "Je suis Jeffrey",  # Trop g√©n√©rique
        "Hello, I am",
        "Error:",
        "STUB:",
        "DEFAULT_RESPONSE",
        "placeholder",
        "not implemented",
        "TODO:",
        "[MOCK]",
        "...",
    ]

    # R√©ponse trop courte = probablement stub
    if len(response) < 20:
        return True

    # Chercher les patterns de stub
    response_lower = response.lower()
    for pattern in stub_patterns:
        if pattern.lower() in response_lower:
            return True

    # R√©ponse identique r√©p√©t√©e = stub
    words = response.split()
    if len(set(words)) < 5:  # Moins de 5 mots uniques
        return True

    return False


async def test_direct_ollama():
    """Test direct avec ApertusClient ‚Üí Ollama"""
    print("\n" + "=" * 70)
    print("üß™ TEST DIRECT OLLAMA ‚Üí JEFFREY")
    print("=" * 70)

    # Cr√©er le client
    print("\nüì¶ Initialisation d'ApertusClient...")
    client = ApertusClient()

    # Questions de test vari√©es
    test_questions = [
        {
            "prompt": "Bonjour, je m'appelle David. Peux-tu te pr√©senter ?",
            "expected": "r√©ponse personnalis√©e avec pr√©nom David",
        },
        {
            "prompt": "Quelle est la capitale de la France et pourquoi est-elle importante ?",
            "expected": "Paris avec explication",
        },
        {
            "prompt": "Raconte-moi une blague courte sur les programmeurs",
            "expected": "blague humoristique",
        },
        {
            "prompt": "Explique en termes simples ce qu'est un r√©seau de neurones",
            "expected": "explication technique simplifi√©e",
        },
        {
            "prompt": "Si tu √©tais un animal, lequel serais-tu et pourquoi ?",
            "expected": "r√©ponse cr√©ative et personnelle",
        },
    ]

    results = []

    for i, test in enumerate(test_questions, 1):
        print(f"\n--- Test {i}/5 ---")
        print(f"üë§ Question: {test['prompt'][:80]}...")
        print(f"üìù Attendu: {test['expected']}")

        try:
            start = time.time()

            # Appel direct √† Ollama via ApertusClient
            response = await client.chat(
                system_prompt="Tu es Jeffrey, un assistant AI intelligent et cr√©atif.",
                user_message=test["prompt"],
                max_tokens=200,
                temperature=0.7,
            )

            elapsed = (time.time() - start) * 1000

            # Extraire le texte de la r√©ponse (chat retourne un tuple)
            if isinstance(response, tuple):
                text = response[0]  # Le premier √©l√©ment est le texte
            elif isinstance(response, dict):
                text = response.get("content", response.get("text", str(response)))
            else:
                text = str(response)

            # V√©rifier si c'est un stub
            is_stub = check_if_stub(text)

            if is_stub:
                print("‚ö†Ô∏è STUB D√âTECT√â!")
                print(f"   R√©ponse: {text[:100]}")
            else:
                print("‚úÖ VRAIE R√âPONSE G√âN√âR√âE!")
                print(f"ü§ñ Jeffrey: {text[:200]}{'...' if len(text) > 200 else ''}")

            print(f"‚è±Ô∏è Temps: {elapsed:.1f}ms")
            print(f"üìè Longueur: {len(text)} caract√®res")

            results.append(
                {
                    "question": test["prompt"][:50],
                    "is_stub": is_stub,
                    "response_length": len(text),
                    "latency_ms": elapsed,
                    "response_preview": text[:100],
                }
            )

        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            results.append(
                {
                    "question": test["prompt"][:50],
                    "is_stub": True,
                    "response_length": 0,
                    "latency_ms": 0,
                    "error": str(e),
                }
            )

        # Petite pause entre les requ√™tes
        await asyncio.sleep(0.5)

    # R√©sum√© des r√©sultats
    print("\n" + "=" * 70)
    print("üìä ANALYSE DES R√âPONSES")
    print("=" * 70)

    total_tests = len(results)
    real_responses = sum(1 for r in results if not r["is_stub"])
    stub_responses = total_tests - real_responses

    print("\nüìà Statistiques:")
    print(f"   Total de tests: {total_tests}")
    print(f"   ‚úÖ Vraies r√©ponses: {real_responses}")
    print(f"   ‚ö†Ô∏è Stubs/Erreurs: {stub_responses}")

    if real_responses > 0:
        avg_length = sum(r["response_length"] for r in results if not r["is_stub"]) / real_responses
        avg_latency = sum(r["latency_ms"] for r in results if not r["is_stub"]) / real_responses
        print(f"   üìè Longueur moyenne: {avg_length:.0f} caract√®res")
        print(f"   ‚è±Ô∏è Latence moyenne: {avg_latency:.1f}ms")

    # D√©tail par question
    print("\nüìã D√©tail par question:")
    for i, r in enumerate(results, 1):
        status = "‚ùå STUB" if r["is_stub"] else "‚úÖ REAL"
        print(f"   {i}. {status} - {r['response_length']} chars - {r.get('latency_ms', 0):.0f}ms")
        if "error" in r:
            print(f"      Erreur: {r['error']}")

    # Diagnostic final
    print("\n" + "=" * 70)
    if real_responses == total_tests:
        print("üéâ SUCC√àS TOTAL - Ollama g√©n√®re de vraies r√©ponses!")
        print("Jeffrey peut maintenant tenir de vraies conversations!")
    elif real_responses > 0:
        print("‚ö†Ô∏è SUCC√àS PARTIEL - Certaines r√©ponses sont g√©n√©r√©es")
        print("V√©rifier la configuration pour am√©liorer la fiabilit√©")
    else:
        print("‚ùå √âCHEC - Aucune vraie r√©ponse g√©n√©r√©e")
        print("Les r√©ponses sont des stubs ou des erreurs")
        print("\nActions recommand√©es:")
        print("1. V√©rifier que Ollama tourne: ollama serve")
        print("2. V√©rifier le mod√®le: ollama list")
        print("3. Tester manuellement: ollama run mistral:7b-instruct")


async def test_conversation_continuity():
    """Test de continuit√© de conversation"""
    print("\n" + "=" * 70)
    print("üß™ TEST DE CONTINUIT√â DE CONVERSATION")
    print("=" * 70)

    client = ApertusClient()

    # Conversation avec contexte
    conversation = [
        "Je m'appelle Alice et j'aime la programmation Python",
        "Quel est mon pr√©nom ?",  # Doit se rappeler "Alice"
        "Et qu'est-ce que j'aime faire ?",  # Doit se rappeler "programmation Python"
    ]

    context = []

    for i, message in enumerate(conversation, 1):
        print(f"\n--- Tour {i} ---")
        print(f"üë§ User: {message}")

        # Construire le prompt avec contexte
        full_prompt = ""
        if context:
            full_prompt = "Contexte de la conversation:\n"
            for turn in context:
                full_prompt += f"User: {turn['user']}\nAssistant: {turn['assistant']}\n"
            full_prompt += f"\nUser: {message}\nAssistant:"
        else:
            full_prompt = message

        try:
            response = await client.chat(
                system_prompt="Tu es Jeffrey. Tu te souviens de la conversation pr√©c√©dente.",
                user_message=message,  # Just the current message, context is in system prompt
                max_tokens=150,
                temperature=0.7,
            )

            # chat retourne un tuple (text, metadata)
            if isinstance(response, tuple):
                text = response[0]
            elif isinstance(response, dict):
                text = response.get("content", response.get("text", str(response)))
            else:
                text = str(response)

            is_stub = check_if_stub(text)

            if not is_stub:
                print(f"ü§ñ Jeffrey: {text[:200]}")

                # V√©rifier la m√©moire contextuelle
                if i == 2 and "alice" in text.lower():
                    print("‚úÖ M√©moire OK - Se souvient du pr√©nom!")
                elif i == 3 and ("python" in text.lower() or "programm" in text.lower()):
                    print("‚úÖ M√©moire OK - Se souvient de l'activit√©!")
            else:
                print(f"‚ö†Ô∏è Stub d√©tect√©: {text[:100]}")

            # Ajouter au contexte
            context.append({"user": message, "assistant": text})

        except Exception as e:
            print(f"‚ùå Erreur: {e}")


async def main():
    """Lance tous les tests"""
    print("\n" + "=" * 70)
    print("üöÄ TEST JEFFREY ‚Üí OLLAMA (D√âTECTION DE STUBS)")
    print("=" * 70)

    # Info syst√®me
    print("\nüìã Configuration:")
    print(f"   Python: {sys.version.split()[0]}")
    print(f"   R√©pertoire: {os.getcwd()}")

    try:
        # Test principal
        await test_direct_ollama()

        # Test de continuit√©
        await test_conversation_continuity()

        print("\n" + "=" * 70)
        print("‚úÖ TESTS TERMIN√âS")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrompus")
    except Exception as e:
        print(f"\n‚ùå Erreur critique: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
