#!/usr/bin/env python3
"""
Jeffrey V2.0 Memory Systems - Architecture Complete
Syst√®me de m√©moire contextuelle intelligent et multi-niveaux

Features:
- M√©moire court/moyen/long terme avec decay naturel
- Tags √©motionnels automatiques via Emotion Engine
- Prioritisation intelligente et recherche contextuelle
- Performance optimis√©e <50ms pour rappel
- Int√©gration seamless avec AGI Orchestrator
"""

import json
import logging
import math
import os
import re
import shutil
import tempfile
import time
import uuid
from collections import OrderedDict, defaultdict
from dataclasses import asdict, dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any

import numpy as np

logger = logging.getLogger(__name__)


class JSONMemoryValidator:
    """
    Validateur JSON robuste avec syst√®me de backup et restoration automatique
    Pr√©vient les erreurs 'Expecting value: line 1 column 1 (char 0)'
    """

    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)

    def safe_load_json(self, filepath: str, backup_filepath: str | None = None) -> dict[str, Any]:
        """
        Charge un fichier JSON avec validation et fallback sur backup

        Args:
            filepath: Chemin vers le fichier JSON principal
            backup_filepath: Chemin vers le fichier de backup (optionnel)

        Returns:
            Dict[str, Any]: Donn√©es JSON ou dictionnaire vide en cas d'erreur
        """
        file_path = Path(filepath)

        # Tentative de chargement du fichier principal
        try:
            if file_path.exists() and file_path.stat().st_size > 0:
                with open(file_path, encoding='utf-8') as f:
                    content = f.read().strip()
                    if content:  # V√©rifier que le contenu n'est pas vide
                        data = json.loads(content)
                        self.logger.debug(f"‚úÖ JSON loaded successfully: {filepath}")
                        return data
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Empty JSON file: {filepath}")

        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON decode error in {filepath}: {e}")
        except Exception as e:
            self.logger.error(f"‚ùå Error reading {filepath}: {e}")

        # Tentative de restoration depuis backup
        if backup_filepath:
            backup_path = Path(backup_filepath)
            try:
                if backup_path.exists() and backup_path.stat().st_size > 0:
                    with open(backup_path, encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            data = json.loads(content)
                            self.logger.info(f"üîÑ Restored from backup: {backup_filepath}")

                            # Restaurer le fichier principal depuis le backup
                            shutil.copy2(backup_path, file_path)
                            self.logger.info(f"üîß Main file restored from backup: {filepath}")
                            return data

            except Exception as e:
                self.logger.error(f"‚ùå Error reading backup {backup_filepath}: {e}")

        # Retourner dictionnaire vide en dernier recours
        self.logger.warning(f"‚ö†Ô∏è Returning empty dict for: {filepath}")
        return {}

    def atomic_save_json(self, data: dict[str, Any], filepath: str, create_backup: bool = True) -> bool:
        """
        Sauvegarde atomique avec backup automatique

        Args:
            data: Donn√©es √† sauvegarder
            filepath: Chemin de destination
            create_backup: Cr√©er un backup avant sauvegarde

        Returns:
            bool: True si succ√®s, False sinon
        """
        file_path = Path(filepath)

        try:
            # Cr√©er le dossier parent si n√©cessaire
            file_path.parent.mkdir(parents=True, exist_ok=True)

            # Cr√©er backup du fichier existant
            if create_backup and file_path.exists():
                backup_path = file_path.with_suffix(f"{file_path.suffix}.backup")
                try:
                    shutil.copy2(file_path, backup_path)
                    self.logger.debug(f"üíæ Backup created: {backup_path}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Could not create backup: {e}")

            # √âcriture atomique via fichier temporaire
            with tempfile.NamedTemporaryFile(
                mode='w',
                encoding='utf-8',
                dir=file_path.parent,
                prefix=f".{file_path.name}.",
                suffix=".tmp",
                delete=False,
            ) as tmp_file:
                json.dump(data, tmp_file, indent=2, ensure_ascii=False)
                tmp_file.flush()
                os.fsync(tmp_file.fileno())  # Force √©criture sur disque
                tmp_path = tmp_file.name

            # Renommage atomique
            shutil.move(tmp_path, file_path)
            self.logger.debug(f"‚úÖ JSON saved atomically: {filepath}")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Error saving JSON {filepath}: {e}")
            # Nettoyer le fichier temporaire si n√©cessaire
            try:
                if 'tmp_path' in locals() and Path(tmp_path).exists():
                    os.unlink(tmp_path)
            except:
                pass
            return False

    def validate_json_structure(self, data: Any, expected_structure: dict | None = None) -> bool:
        """
        Valide la structure JSON contre un sch√©ma attendu

        Args:
            data: Donn√©es √† valider
            expected_structure: Structure attendue (optionnel)

        Returns:
            bool: True si structure valide
        """
        try:
            if not isinstance(data, dict):
                return False

            if expected_structure:
                for key, expected_type in expected_structure.items():
                    if key not in data:
                        self.logger.warning(f"‚ö†Ô∏è Missing key in JSON: {key}")
                        return False
                    if not isinstance(data[key], expected_type):
                        self.logger.warning(f"‚ö†Ô∏è Wrong type for {key}: expected {expected_type}, got {type(data[key])}")
                        return False

            return True

        except Exception as e:
            self.logger.error(f"‚ùå JSON validation error: {e}")
            return False

    def repair_json_file(self, filepath: str) -> bool:
        """
        Tente de r√©parer un fichier JSON corrompu

        Args:
            filepath: Chemin vers le fichier √† r√©parer

        Returns:
            bool: True si r√©paration r√©ussie
        """
        file_path = Path(filepath)

        try:
            if not file_path.exists():
                return False

            with open(file_path, encoding='utf-8') as f:
                content = f.read()

            # Tentatives de r√©paration courantes
            repairs = [
                lambda x: x.strip(),  # Supprimer espaces
                lambda x: x.rstrip(',\n\r\t ') + '\n',  # Supprimer virgule finale
                lambda x: x + '}' if x.count('{') > x.count('}') else x,  # Ajouter } manquant
                lambda x: x + ']' if x.count('[') > x.count(']') else x,  # Ajouter ] manquant
            ]

            for repair_func in repairs:
                try:
                    repaired_content = repair_func(content)
                    json.loads(repaired_content)  # Test si valide

                    # Sauvegarder la version r√©par√©e
                    self.atomic_save_json(json.loads(repaired_content), filepath, create_backup=True)
                    self.logger.info(f"üîß JSON file repaired: {filepath}")
                    return True

                except json.JSONDecodeError:
                    continue

            self.logger.error(f"‚ùå Could not repair JSON file: {filepath}")
            return False

        except Exception as e:
            self.logger.error(f"‚ùå Error repairing JSON file {filepath}: {e}")
            return False


class MemoryLevel(Enum):
    """Niveaux de m√©moire avec caract√©ristiques sp√©cifiques"""

    SHORT_TERM = "short_term"  # 0-24h, haute volatilit√©
    MEDIUM_TERM = "medium_term"  # 1-30 jours, decay mod√©r√©
    LONG_TERM = "long_term"  # 30+ jours, stable, compress√©


class MemoryType(Enum):
    """Types de contenu m√©moire"""

    DIALOGUE = "dialogue"
    EMOTIONAL = "emotional"
    FACTUAL = "factual"
    CREATIVE = "creative"
    RELATIONSHIP = "relationship"
    LEARNING = "learning"


@dataclass
class EmotionalTag:
    """Tag √©motionnel enrichi par Emotion Engine"""

    emotion: str
    intensity: float  # 0.0 - 1.0
    valence: float  # -1.0 (n√©gatif) √† +1.0 (positif)
    arousal: float  # 0.0 (calme) √† 1.0 (excit√©)
    confidence: float  # 0.0 - 1.0
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class ContextVector:
    """Vecteur de contexte pour recherche s√©mantique"""

    keywords: list[str] = field(default_factory=list)
    topics: list[str] = field(default_factory=list)
    entities: list[str] = field(default_factory=list)
    semantic_hash: str = ""
    similarity_threshold: float = 0.7


@dataclass
class MemoryEntry:
    """Structure compl√®te d'une entr√©e m√©moire"""

    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    content: str = ""
    memory_type: MemoryType = MemoryType.DIALOGUE
    memory_level: MemoryLevel = MemoryLevel.SHORT_TERM

    # Enrichissement √©motionnel
    emotional_tags: list[EmotionalTag] = field(default_factory=list)
    emotional_weight: float = 0.0  # Impact √©motionnel global

    # M√©tadonn√©es contextuelles
    context_vector: ContextVector = field(default_factory=ContextVector)
    user_id: str = "default"
    conversation_id: str = ""

    # Scoring et priorit√©s
    importance_score: float = 0.5  # 0.0 - 1.0
    access_count: int = 0
    last_accessed: str = field(default_factory=lambda: datetime.now().isoformat())

    # Decay et compression
    decay_factor: float = 1.0  # 1.0 = frais, 0.0 = totalement d√©grad√©
    compressed: bool = False
    compression_ratio: float = 1.0

    # Relations et liens
    related_memories: list[str] = field(default_factory=list)  # IDs des m√©moires li√©es
    parent_memory: str | None = None
    child_memories: list[str] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        """Convertit l'entr√©e m√©moire en dictionnaire JSON-s√©rialisable"""
        return {
            'id': self.id,
            'timestamp': self.timestamp,
            'content': self.content,
            'memory_type': self.memory_type.value,  # Convertir Enum en string
            'memory_level': self.memory_level.value,  # Convertir Enum en string
            'emotional_tags': [asdict(tag) for tag in self.emotional_tags],
            'emotional_weight': self.emotional_weight,
            'context_vector': asdict(self.context_vector),
            'user_id': self.user_id,
            'conversation_id': self.conversation_id,
            'importance_score': self.importance_score,
            'access_count': self.access_count,
            'last_accessed': self.last_accessed,
            'decay_factor': self.decay_factor,
            'compressed': self.compressed,
            'compression_ratio': self.compression_ratio,
            'related_memories': self.related_memories,
            'parent_memory': self.parent_memory,
            'child_memories': self.child_memories,
        }


class MemoryCore:
    """
    Syst√®me de m√©moire contextuelle intelligent pour Jeffrey V2.0

    Architecture multi-niveaux avec gestion √©motionnelle avanc√©e,
    decay naturel et optimisation de performance.
    """

    def __init__(self, config_path: str = "data/memory_config.json"):
        """
        Initialise le syst√®me de m√©moire

        Args:
            config_path: Chemin vers le fichier de configuration
        """
        # Initialiser le validateur JSON robuste
        self.json_validator = JSONMemoryValidator(logger)

        self.config = self._load_config(config_path)
        self.memories: dict[str, MemoryEntry] = {}
        self.memory_index: dict[MemoryLevel, set[str]] = {level: set() for level in MemoryLevel}
        self.emotional_index: dict[str, set[str]] = defaultdict(set)
        self.topic_index: dict[str, set[str]] = defaultdict(set)
        self.user_index: dict[str, set[str]] = defaultdict(set)

        # M√©triques de performance
        self.stats = {
            "total_memories": 0,
            "by_level": {level.value: 0 for level in MemoryLevel},
            "by_type": {mtype.value: 0 for mtype in MemoryType},
            "avg_access_time": 0.0,
            "compression_ratio": 0.0,
            "last_cleanup": datetime.now().isoformat(),
        }

        # Int√©gration avec autres syst√®mes
        self.emotion_engine = None
        self.agi_orchestrator = None

        # Cache LRU optimis√© pour performance
        self._search_cache: OrderedDict[str, tuple[list[MemoryEntry], float]] = OrderedDict()
        self._cache_ttl = self.config.get("cache_ttl", 300)  # 5 minutes
        self._max_cache_size = self.config.get("max_cache_size", 1000)
        self._query_analytics = defaultdict(int)  # Analyse fr√©quence des requ√™tes

        # Optimisations avanc√©es
        self._similarity_cache: dict[str, float] = {}
        self._precomputed_vectors: dict[str, np.ndarray] = {}
        self._frequent_patterns: set[str] = set()

        logger.info("üß† MemoryCore initialized with robust JSON validation - Ready for contextual intelligence")

    def _load_config(self, config_path: str) -> dict[str, Any]:
        """Charge la configuration du syst√®me m√©moire avec validation JSON robuste"""
        default_config = {
            "max_memories": 15000,  # Augment√© pour de meilleures performances
            "decay_rates": {
                "short_term": 0.05,  # R√©duit de 0.1 √† 0.05 - pr√©servation accrue
                "medium_term": 0.02,  # R√©duit de 0.05 √† 0.02 - pr√©servation forte
                "long_term": 0.005,  # R√©duit de 0.01 √† 0.005 - pr√©servation maximale
            },
            "promotion_thresholds": {
                "short_to_medium": 0.65,  # R√©duit de 0.7 √† 0.65 - promotion plus facile
                "medium_to_long": 0.75,  # R√©duit de 0.8 √† 0.75 - promotion plus facile
            },
            "compression_threshold": 0.25,  # R√©duit de 0.3 √† 0.25 - compression plus tardive
            "cleanup_interval": 3600,  # 1 heure
            "performance_target": 0.03,  # Optimis√© de 0.05 √† 0.03 (30ms)
            "emotional_weight_multiplier": 2.5,  # Augment√© de 2.0 √† 2.5 - impact √©motionnel renforc√©
            "cache_ttl": 600,  # Augment√© de 300 √† 600 (10 minutes)
            "max_cache_size": 2000,  # Doubl√© pour meilleures performances
            "similarity_threshold": 0.80,  # Augment√© de 0.75 √† 0.80 pour >90% pr√©cision
            "semantic_boost_factor": 2.2,  # Augment√© de 1.8 √† 2.2 - boost s√©mantique renforc√©
            "recency_decay_factor": 0.90,  # Augment√© de 0.85 √† 0.90 - importance de la r√©cence
            "batching_enabled": True,
            "batch_size": 100,  # Augment√© de 50 √† 100 pour efficacit√©
            "quality_threshold": 0.70,  # Nouveau: seuil de qualit√© pour le recall
            "context_expansion_factor": 1.5,  # Nouveau: facteur d'expansion du contexte
            "emotional_memory_boost": 1.8,  # Nouveau: boost sp√©cifique pour m√©moires √©motionnelles
        }

        try:
            config_file = Path(config_path)
            backup_path = config_file.with_suffix(f"{config_file.suffix}.backup")

            if config_file.exists():
                # Charger avec validation JSON robuste
                user_config = self.json_validator.safe_load_json(
                    str(config_file), str(backup_path) if backup_path.exists() else None
                )

                if user_config:  # Si le fichier a √©t√© charg√© avec succ√®s
                    default_config.update(user_config)
                    logger.info(f"‚úÖ Configuration loaded: {config_path}")
                else:
                    # Cr√©er un nouveau fichier de config
                    logger.warning("‚ö†Ô∏è Creating new config file with defaults")
                    config_file.parent.mkdir(parents=True, exist_ok=True)
                    self.json_validator.atomic_save_json(default_config, str(config_file))
            else:
                # Cr√©er le fichier de config par d√©faut avec sauvegarde atomique
                config_file.parent.mkdir(parents=True, exist_ok=True)
                success = self.json_validator.atomic_save_json(default_config, str(config_file))
                if success:
                    logger.info(f"‚úÖ Default config created: {config_path}")
                else:
                    logger.error(f"‚ùå Failed to create config file: {config_path}")

        except Exception as e:
            logger.error(f"‚ùå Critical error loading config: {e}, using defaults")

        return default_config

    def integrate_emotion_engine(self, emotion_engine):
        """Int√®gre l'Emotion Engine pour enrichissement automatique"""
        self.emotion_engine = emotion_engine
        logger.info("üé≠ Emotion Engine integrated with Memory Systems")

    def integrate_agi_orchestrator(self, agi_orchestrator):
        """Int√®gre l'AGI Orchestrator pour capture automatique"""
        self.agi_orchestrator = agi_orchestrator
        logger.info("üéØ AGI Orchestrator integrated with Memory Systems")

    async def store_memory(
        self,
        content: str,
        memory_type: MemoryType = MemoryType.DIALOGUE,
        user_id: str = "default",
        conversation_id: str = "",
        metadata: dict[str, Any] | None = None,
    ) -> str:
        """
        Stocke une nouvelle m√©moire avec enrichissement automatique

        Args:
            content: Contenu √† m√©moriser
            memory_type: Type de m√©moire
            user_id: ID utilisateur
            conversation_id: ID conversation
            metadata: M√©tadonn√©es additionnelles

        Returns:
            str: ID de la m√©moire cr√©√©e
        """
        start_time = time.time()

        try:
            # Cr√©er l'entr√©e m√©moire
            memory = MemoryEntry(
                content=content, memory_type=memory_type, user_id=user_id, conversation_id=conversation_id
            )

            # Enrichissement √©motionnel via Emotion Engine
            if self.emotion_engine:
                emotional_analysis = await self._analyze_emotional_content(content)
                memory.emotional_tags = emotional_analysis["tags"]
                memory.emotional_weight = emotional_analysis["weight"]

            # G√©n√©ration du vecteur de contexte
            memory.context_vector = self._generate_context_vector(content, metadata)

            # Calcul de l'importance
            memory.importance_score = self._calculate_importance(memory)

            # D√©termination du niveau initial
            memory.memory_level = self._determine_initial_level(memory)

            # Stockage et indexation
            self.memories[memory.id] = memory
            self._update_indices(memory)

            # Mise √† jour des statistiques
            self._update_stats(memory)

            # Nettoyage intelligent avec batching
            if len(self.memories) > self.config["max_memories"]:
                if self.config.get("batching_enabled", True):
                    await self._batch_cleanup_memories()
                else:
                    await self._cleanup_old_memories()

            processing_time = time.time() - start_time
            self.stats["avg_access_time"] = self.stats["avg_access_time"] * 0.9 + processing_time * 0.1

            logger.debug(f"üíæ Memory stored: {memory.id[:8]} ({processing_time:.3f}s)")
            return memory.id

        except Exception as e:
            logger.error(f"Erreur stockage m√©moire: {e}")
            raise

    async def recall_contextual(
        self,
        query: str,
        user_id: str = "default",
        limit: int = 5,
        emotional_filter: str | None = None,
        time_range: tuple[datetime, datetime] | None = None,
        quality_threshold: float | None = None,
    ) -> list[MemoryEntry]:
        """
        Rappel contextuel intelligent des m√©moires

        Args:
            query: Requ√™te de recherche
            user_id: ID utilisateur pour filtrer
            limit: Nombre max de r√©sultats
            emotional_filter: Filtre par √©motion
            time_range: Plage temporelle

        Returns:
            List[MemoryEntry]: M√©moires pertinentes tri√©es par relevance
        """
        start_time = time.time()

        try:
            # Analyser la fr√©quence des requ√™tes pour optimisation
            self._query_analytics[query] += 1

            # V√©rifier le cache LRU optimis√©
            cache_key = f"{query}_{user_id}_{limit}_{emotional_filter}"
            if cache_key in self._search_cache:
                cached_result, cache_time = self._search_cache[cache_key]
                if time.time() - cache_time < self._cache_ttl:
                    # D√©placer vers la fin pour LRU
                    self._search_cache.move_to_end(cache_key)
                    return cached_result[:limit]

            # G√©n√©ration du vecteur de requ√™te
            query_vector = self._generate_context_vector(query)

            # Recherche multi-crit√®res
            candidates = self._find_candidate_memories(user_id, emotional_filter, time_range)

            # Scoring s√©mantique et contextuel avec filtrage qualit√©
            scored_memories = []
            quality_threshold = quality_threshold or self.config.get("quality_threshold", 0.70)

            for memory_id in candidates:
                memory = self.memories[memory_id]
                score = self._calculate_relevance_score(memory, query_vector, query)

                # Appliquer le seuil de qualit√© pour >90% pr√©cision
                if score >= quality_threshold:
                    scored_memories.append((score, memory))

            # Tri par pertinence
            scored_memories.sort(key=lambda x: x[0], reverse=True)

            # Expansion adaptative si pas assez de r√©sultats de qualit√©
            if len(scored_memories) < limit and quality_threshold > 0.3:
                # R√©duire le seuil progressivement pour avoir des r√©sultats
                fallback_threshold = max(0.3, quality_threshold - 0.2)

                for memory_id in candidates:
                    memory = self.memories[memory_id]
                    score = self._calculate_relevance_score(memory, query_vector, query)

                    if fallback_threshold <= score < quality_threshold:
                        scored_memories.append((score, memory))

                scored_memories.sort(key=lambda x: x[0], reverse=True)

            results = [memory for _, memory in scored_memories[:limit]]

            # Mise √† jour des acc√®s
            for memory in results:
                memory.access_count += 1
                memory.last_accessed = datetime.now().isoformat()

            # Cache du r√©sultat avec gestion LRU
            self._search_cache[cache_key] = (results, time.time())
            self._search_cache.move_to_end(cache_key)

            # Nettoyage du cache si trop grand
            while len(self._search_cache) > self._max_cache_size:
                self._search_cache.popitem(last=False)

            processing_time = time.time() - start_time
            logger.debug(f"üîç Contextual recall: {len(results)} memories ({processing_time:.3f}s)")

            return results

        except Exception as e:
            logger.error(f"Erreur rappel contextuel: {e}")
            return []

    def get_memory_by_id(self, memory_id: str) -> MemoryEntry | None:
        """R√©cup√®re une m√©moire par son ID"""
        memory = self.memories.get(memory_id)
        if memory:
            memory.access_count += 1
            memory.last_accessed = datetime.now().isoformat()
        return memory

    async def update_memory(self, memory_id: str, updates: dict[str, Any]) -> bool:
        """Met √† jour une m√©moire existante"""
        try:
            if memory_id not in self.memories:
                return False

            memory = self.memories[memory_id]
            old_level = memory.memory_level

            # Appliquer les mises √† jour
            for key, value in updates.items():
                if hasattr(memory, key):
                    setattr(memory, key, value)

            # Re-indexer si n√©cessaire
            if memory.memory_level != old_level:
                self.memory_index[old_level].discard(memory_id)
                self.memory_index[memory.memory_level].add(memory_id)

            logger.debug(f"üìù Memory updated: {memory_id[:8]}")
            return True

        except Exception as e:
            logger.error(f"Erreur mise √† jour m√©moire: {e}")
            return False

    async def delete_memory(self, memory_id: str) -> bool:
        """Supprime une m√©moire"""
        try:
            if memory_id not in self.memories:
                return False

            memory = self.memories[memory_id]

            # Supprimer des index
            self._remove_from_indices(memory)

            # Supprimer la m√©moire
            del self.memories[memory_id]

            # Mettre √† jour les stats
            self.stats["total_memories"] -= 1
            self.stats["by_level"][memory.memory_level.value] -= 1
            self.stats["by_type"][memory.memory_type.value] -= 1

            logger.debug(f"üóëÔ∏è Memory deleted: {memory_id[:8]}")
            return True

        except Exception as e:
            logger.error(f"Erreur suppression m√©moire: {e}")
            return False

    async def process_memory_decay(self) -> dict[str, int]:
        """Traite le decay naturel des m√©moires"""
        stats = {"decayed": 0, "promoted": 0, "compressed": 0, "deleted": 0}

        try:
            current_time = datetime.now()

            for memory_id, memory in list(self.memories.items()):
                memory_age = current_time - datetime.fromisoformat(memory.timestamp)

                # Calcul du decay bas√© sur l'√¢ge et le niveau
                decay_rate = self.config["decay_rates"][memory.memory_level.value]
                age_factor = memory_age.days * decay_rate

                # Facteur de protection √©motionnelle renforc√©
                emotional_protection = memory.emotional_weight * self.config["emotional_weight_multiplier"]

                # Protection additionnelle pour m√©moires importantes
                importance_protection = memory.importance_score * 0.3

                # Protection pour m√©moires fr√©quemment acc√©d√©es
                access_protection = min(0.2, memory.access_count / 15.0)

                # Application du decay avec protections multiples
                total_protection = emotional_protection + importance_protection + access_protection
                new_decay = max(0.0, memory.decay_factor - age_factor + total_protection)
                memory.decay_factor = new_decay

                # Gestion des transitions de niveau
                if self._should_promote_memory(memory):
                    await self._promote_memory(memory)
                    stats["promoted"] += 1
                elif self._should_compress_memory(memory):
                    await self._compress_memory(memory)
                    stats["compressed"] += 1
                elif self._should_delete_memory(memory):
                    await self.delete_memory(memory_id)
                    stats["deleted"] += 1
                else:
                    stats["decayed"] += 1

            self.stats["last_cleanup"] = current_time.isoformat()
            logger.info(f"üßπ Memory decay processed: {stats}")

        except Exception as e:
            logger.error(f"Erreur traitement decay: {e}")

        return stats

    def get_memory_stats(self) -> dict[str, Any]:
        """Retourne les statistiques d√©taill√©es du syst√®me m√©moire"""
        return {
            **self.stats,
            "memory_levels": {level.value: len(memories) for level, memories in self.memory_index.items()},
            "cache_size": len(self._search_cache),
            "index_sizes": {
                "emotional": len(self.emotional_index),
                "topic": len(self.topic_index),
                "user": len(self.user_index),
            },
        }

    def export_memories(
        self,
        user_id: str | None = None,
        time_range: tuple[datetime, datetime] | None = None,
        format: str = "json",
        compressed: bool = False,
    ) -> str:
        """Exporte les m√©moires au format sp√©cifi√© avec optimisations"""
        try:
            memories_to_export = []

            # Lazy loading avec g√©n√©rateur pour √©conomiser la m√©moire
            for memory in self._get_filtered_memories(user_id, time_range):
                if compressed:
                    # Export compress√© avec seulement les champs essentiels
                    memory_data = {
                        'id': memory.id,
                        'content': memory.content[:200] if len(memory.content) > 200 else memory.content,
                        'timestamp': memory.timestamp,
                        'importance': memory.importance_score,
                        'emotional_weight': memory.emotional_weight,
                        'topics': memory.context_vector.topics,
                    }
                else:
                    memory_data = memory.to_dict()

                memories_to_export.append(memory_data)

            if format == "json":
                return json.dumps(memories_to_export, indent=2 if not compressed else None, ensure_ascii=False)
            else:
                raise ValueError(f"Format non support√©: {format}")

        except Exception as e:
            logger.error(f"Erreur export m√©moires: {e}")
            return ""

    def _get_filtered_memories(self, user_id: str | None, time_range: tuple[datetime, datetime] | None):
        """G√©n√©rateur pour lazy loading des m√©moires filtr√©es"""
        for memory in self.memories.values():
            # Filtrer par utilisateur
            if user_id and memory.user_id != user_id:
                continue

            # Filtrer par plage temporelle
            if time_range:
                memory_time = datetime.fromisoformat(memory.timestamp)
                if not (time_range[0] <= memory_time <= time_range[1]):
                    continue

            yield memory

    async def _analyze_emotional_content(self, content: str) -> dict[str, Any]:
        """Analyse √©motionnelle du contenu via Emotion Engine"""
        try:
            if hasattr(self.emotion_engine, 'analyze_emotion_hybrid'):
                analysis = self.emotion_engine.analyze_emotion_hybrid(content)

                emotional_tag = EmotionalTag(
                    emotion=analysis.get('emotion_dominante', 'neutre'),
                    intensity=analysis.get('intensite', 50) / 100.0,
                    valence=analysis.get('contexte_emotionnel', {}).get('valence', 0.0),
                    arousal=analysis.get('contexte_emotionnel', {}).get('arousal', 0.0),
                    confidence=analysis.get('confiance', 50) / 100.0,
                )

                emotional_weight = (
                    emotional_tag.intensity * 0.4
                    + abs(emotional_tag.valence) * 0.3
                    + emotional_tag.arousal * 0.2
                    + emotional_tag.confidence * 0.1
                )

                return {"tags": [emotional_tag], "weight": emotional_weight}
            else:
                return {"tags": [], "weight": 0.0}

        except Exception as e:
            logger.warning(f"Erreur analyse √©motionnelle: {e}")
            return {"tags": [], "weight": 0.0}

    def _generate_context_vector(self, content: str, metadata: dict | None = None) -> ContextVector:
        """G√©n√®re un vecteur de contexte optimis√© pour recherche s√©mantique pr√©cise"""
        try:
            # Normalisation et nettoyage avanc√©
            normalized_content = self._normalize_text(content)

            # Extraction de mots-cl√©s avec scoring TF-IDF simplifi√©
            keywords = self._extract_weighted_keywords(normalized_content)

            # D√©tection de topics avec patterns √©tendus et scoring
            topics = self._detect_semantic_topics(normalized_content)

            # Extraction d'entit√©s nomm√©es simples
            entities = self._extract_entities(normalized_content)

            # Hash s√©mantique bas√© sur les concepts cl√©s
            semantic_concepts = keywords + topics + entities
            semantic_hash = self._generate_semantic_hash(semantic_concepts)

            return ContextVector(
                keywords=keywords[:15],  # Plus de mots-cl√©s pour meilleure pr√©cision
                topics=topics,
                entities=entities,
                semantic_hash=semantic_hash,
                similarity_threshold=self.config.get("similarity_threshold", 0.75),
            )

        except Exception as e:
            logger.warning(f"Erreur g√©n√©ration vecteur contexte: {e}")
            return ContextVector()

    def _normalize_text(self, text: str) -> str:
        """Normalise le texte pour am√©liorer la recherche"""
        # Conversion en minuscules
        text = text.lower()
        # Suppression de la ponctuation excessive
        text = re.sub(r'[^\w\s]', ' ', text)
        # Normalisation des espaces
        text = ' '.join(text.split())
        return text

    def _extract_weighted_keywords(self, text: str) -> list[str]:
        """Extrait les mots-cl√©s avec scoring de fr√©quence"""
        words = text.split()

        # Filtrage intelligent des mots (liste r√©duite pour capturer plus de mots-cl√©s)
        stop_words = {
            'le',
            'la',
            'les',
            'un',
            'une',
            'des',
            'du',
            'de',
            'et',
            'ou',
            'mais',
            'que',
            'qui',
            'est',
            'sont',
            'avoir',
            '√™tre',
        }

        # Scoring bas√© sur longueur et raret√©
        word_scores = {}
        for word in words:
            if len(word) >= 3 and word.isalpha() and word not in stop_words:
                # Score bas√© sur longueur et position
                score = len(word) * 0.5
                if word in word_scores:
                    word_scores[word] += score * 0.8  # Bonus r√©p√©tition mod√©r√©
                else:
                    word_scores[word] = score

        # Tri par score et s√©lection des meilleurs
        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
        return [word for word, score in sorted_words[:15]]

    def _detect_semantic_topics(self, text: str) -> list[str]:
        """D√©tection avanc√©e de topics avec patterns √©tendus"""
        topic_patterns = {
            "√©motion": [
                r'\b(ressens|sens|√©motion|sentiment|tristesse|joie|col√®re|peur|amour|haine|bonheur)\b',
                r'\b(heureux|triste|en col√®re|effray√©|amoureux|content|d√©prim√©)\b',
            ],
            "cr√©ativit√©": [
                r'\b(cr√©ativit√©|cr√©atif|cr√©e|cr√©er|imagine|invente|histoire|art|dessin|musique|√©criture)\b',
                r'\b(inspiration|id√©e|projet|cr√©ation|artistique|original|collaboration|collaborer)\b',
            ],
            "relation": [
                r'\b(amour|amiti√©|ensemble|famille|ami|copain|relation|couple)\b',
                r'\b(toi|moi|nous|vous|ils|elles|rencontre|partage)\b',
            ],
            "technique": [
                r'\b(code|programme|bug|fonction|algorithme|d√©veloppement|ordinateur)\b',
                r'\b(python|javascript|html|css|donn√©es|base|serveur)\b',
            ],
            "apprentissage": [
                r'\b(apprendre|√©tudier|comprendre|savoir|connaissance|le√ßon)\b',
                r'\b(√©cole|universit√©|cours|formation|enseignement)\b',
            ],
            "travail": [
                r'\b(travail|job|emploi|carri√®re|bureau|coll√®gue|patron|projet)\b',
                r'\b(r√©union|t√¢che|deadline|objectif|performance)\b',
            ],
            "sant√©": [
                r'\b(sant√©|maladie|m√©decin|h√¥pital|traitement|douleur|fatigue)\b',
                r'\b(sport|exercice|fitness|alimentation|nutrition)\b',
            ],
            "voyage": [
                r'\b(voyage|vacances|pays|ville|transport|avion|train|voiture)\b',
                r'\b(d√©couverte|exploration|culture|langue|restaurant)\b',
            ],
        }

        detected_topics = []
        for topic, patterns in topic_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    detected_topics.append(topic)
                    break  # Une seule occurrence par topic

        return detected_topics

    def _extract_entities(self, text: str) -> list[str]:
        """Extraction d'entit√©s nomm√©es simples"""
        entities = []

        # Noms propres (mots commen√ßant par une majuscule)
        words = text.split()
        for word in words:
            if len(word) > 2 and word[0].isupper() and word[1:].islower():
                entities.append(word.lower())

        # Dates et nombres
        date_pattern = r'\b(\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4})\b'
        number_pattern = r'\b\d+\b'

        entities.extend(['date'] * len(re.findall(date_pattern, text)))
        if re.search(number_pattern, text):
            entities.append('nombre')

        return list(set(entities))[:5]  # Limiter √† 5 entit√©s max

    def _generate_semantic_hash(self, concepts: list[str]) -> str:
        """G√©n√®re un hash s√©mantique bas√© sur les concepts"""
        if not concepts:
            return str(hash('empty'))[:16]

        # Tri pour coh√©rence
        sorted_concepts = sorted(set(concepts))
        combined = ''.join(sorted_concepts)
        return str(hash(combined))[:16]

    def _calculate_importance(self, memory: MemoryEntry) -> float:
        """Calcule le score d'importance optimis√© d'une m√©moire"""
        try:
            importance = 0.4  # Base l√©g√®rement r√©duite

            # Facteur √©motionnel renforc√©
            if memory.emotional_tags:
                emotional_scores = []
                for tag in memory.emotional_tags:
                    # Score composite : intensit√© √ó confiance √ó |valence|
                    tag_score = tag.intensity * tag.confidence * (abs(tag.valence) * 0.5 + 0.5)
                    emotional_scores.append(tag_score)

                avg_emotional_score = sum(emotional_scores) / len(emotional_scores)
                importance += avg_emotional_score * 0.4  # Augment√© de 0.3 √† 0.4

            # Facteur de richesse du contenu optimis√©
            content_length = len(memory.content)
            if content_length > 0:
                # Fonction logarithmique pour √©viter de sur-valoriser les textes tr√®s longs
                content_factor = min(math.log(content_length + 1) / math.log(500), 1.0) * 0.25
                importance += content_factor

            # Facteur de diversit√© contextuelle
            context_diversity = 0
            if memory.context_vector.topics:
                context_diversity += len(memory.context_vector.topics) * 0.1
            if memory.context_vector.entities:
                context_diversity += len(memory.context_vector.entities) * 0.05
            importance += min(context_diversity, 0.2)

            # Facteur de type optimis√©
            type_weights = {
                MemoryType.EMOTIONAL: 1.2,  # Augment√© pour priorit√© √©motions
                MemoryType.RELATIONSHIP: 1.1,  # Augment√© pour relations
                MemoryType.CREATIVE: 1.0,  # √âquilibr√©
                MemoryType.LEARNING: 0.9,  # L√©g√®rement r√©duit
                MemoryType.DIALOGUE: 0.8,  # R√©duit car plus commun
                MemoryType.FACTUAL: 0.7,  # Plus faible car moins engageant
            }
            importance *= type_weights.get(memory.memory_type, 0.8)

            # Bonus pour unicit√© (√©viter la redondance)
            if memory.context_vector.semantic_hash:
                # Petit bonus pour les contenus uniques
                importance += 0.05

            return min(1.0, max(0.1, importance))  # Min √† 0.1 au lieu de 0.0

        except Exception as e:
            logger.warning(f"Erreur calcul importance: {e}")
            return 0.5

    def _determine_initial_level(self, memory: MemoryEntry) -> MemoryLevel:
        """D√©termine le niveau initial de la m√©moire"""
        if memory.importance_score > 0.8:
            return MemoryLevel.MEDIUM_TERM
        else:
            return MemoryLevel.SHORT_TERM

    def _update_indices(self, memory: MemoryEntry):
        """Met √† jour tous les index avec la nouvelle m√©moire"""
        # Index par niveau
        self.memory_index[memory.memory_level].add(memory.id)

        # Index √©motionnel
        for tag in memory.emotional_tags:
            self.emotional_index[tag.emotion].add(memory.id)

        # Index par topics
        for topic in memory.context_vector.topics:
            self.topic_index[topic].add(memory.id)

        # Index utilisateur
        self.user_index[memory.user_id].add(memory.id)

    def _remove_from_indices(self, memory: MemoryEntry):
        """Supprime une m√©moire de tous les index"""
        # Supprimer de tous les index
        for index_dict in [self.memory_index, self.emotional_index, self.topic_index, self.user_index]:
            for memory_set in index_dict.values():
                memory_set.discard(memory.id)

    def _update_stats(self, memory: MemoryEntry):
        """Met √† jour les statistiques"""
        self.stats["total_memories"] += 1
        self.stats["by_level"][memory.memory_level.value] += 1
        self.stats["by_type"][memory.memory_type.value] += 1

    def _find_candidate_memories(
        self, user_id: str, emotional_filter: str | None, time_range: tuple[datetime, datetime] | None
    ) -> set[str]:
        """Trouve les m√©moires candidates pour la recherche"""
        candidates = set()

        # Filtrer par utilisateur
        if user_id in self.user_index:
            candidates = self.user_index[user_id].copy()
        else:
            candidates = set(self.memories.keys())

        # Filtrer par √©motion
        if emotional_filter and emotional_filter in self.emotional_index:
            candidates &= self.emotional_index[emotional_filter]

        # Filtrer par temps
        if time_range:
            time_filtered = set()
            for memory_id in candidates:
                memory = self.memories[memory_id]
                memory_time = datetime.fromisoformat(memory.timestamp)
                if time_range[0] <= memory_time <= time_range[1]:
                    time_filtered.add(memory_id)
            candidates = time_filtered

        return candidates

    def _calculate_relevance_score(self, memory: MemoryEntry, query_vector: ContextVector, query: str) -> float:
        """Calcule le score de pertinence optimis√© avec algorithme hybride"""
        try:
            # Composantes du score avec poids optimis√©s
            semantic_score = self._calculate_semantic_similarity(memory, query_vector, query)
            emotional_score = self._calculate_emotional_relevance(memory, query)
            contextual_score = self._calculate_contextual_relevance(memory, query_vector)
            temporal_score = self._calculate_temporal_relevance(memory)
            importance_score = memory.importance_score

            # Combinaison pond√©r√©e optimis√©e pour >90% pr√©cision
            total_score = (
                semantic_score * 0.50  # S√©mantique prioritaire pour pr√©cision
                + emotional_score * 0.20  # √âmotionnel important
                + contextual_score * 0.15  # Contexte structurel
                + temporal_score * 0.10  # Fra√Æcheur mod√©r√©e
                + importance_score * 0.05  # Importance r√©duite
            )

            # Boost par facteurs de qualit√©
            quality_multiplier = self._calculate_quality_multiplier(memory)
            total_score *= quality_multiplier

            # Normalisation finale
            total_score = min(1.0, max(0.0, total_score))

            return total_score

        except Exception as e:
            logger.warning(f"Erreur calcul pertinence: {e}")
            return 0.0

    def _calculate_semantic_similarity(self, memory: MemoryEntry, query_vector: ContextVector, query: str) -> float:
        """Calcule la similarit√© s√©mantique avanc√©e avec approche fuzzy robuste pour >90% pr√©cision"""
        score = 0.0
        content_lower = memory.content.lower()
        query_lower = query.lower()

        # 1. Correspondance directe (exacte ou partielle)
        if query_lower in content_lower:
            position_factor = 1.0 - (content_lower.find(query_lower) / len(content_lower)) * 0.15
            score += 0.9 * position_factor

        # 2. Correspondance avanc√©e de mots avec tolerance fuzzy
        query_words = [w.strip() for w in query_lower.split() if len(w.strip()) >= 2]
        content_words = [w.strip() for w in content_lower.split()]

        if query_words:
            matched_score = 0
            for query_word in query_words:
                best_match_score = 0

                # Chercher correspondance exacte
                for content_word in content_words:
                    if query_word == content_word:
                        best_match_score = 1.0
                        break
                    # Correspondance partielle (pr√©fixe/suffixe)
                    elif query_word in content_word or content_word in query_word:
                        overlap = len(set(query_word) & set(content_word)) / len(set(query_word) | set(content_word))
                        best_match_score = max(best_match_score, overlap * 0.8)
                    # Correspondance floue par caract√®res communs
                    elif len(query_word) >= 4 and len(content_word) >= 4:
                        common_chars = len(set(query_word) & set(content_word))
                        char_similarity = common_chars / max(len(query_word), len(content_word))
                        if char_similarity >= 0.6:
                            best_match_score = max(best_match_score, char_similarity * 0.6)

                # Pond√©rer par importance du mot (longueur)
                word_importance = min(1.0, len(query_word) / 8.0)
                matched_score += best_match_score * word_importance

            word_similarity = matched_score / len(query_words)
            score += word_similarity * 0.8

        # 3. Correspondance de mots-cl√©s extraits avec expansion
        memory_keywords = set(kw.lower() for kw in memory.context_vector.keywords)
        query_keywords = set(kw.lower() for kw in query_vector.keywords)

        if memory_keywords and query_keywords:
            keyword_matches = 0
            for qkw in query_keywords:
                for mkw in memory_keywords:
                    if qkw == mkw or qkw in mkw or mkw in qkw:
                        keyword_matches += 1
                        break

            if keyword_matches > 0:
                keyword_score = keyword_matches / len(query_keywords)
                score += keyword_score * 0.7

        # 4. Correspondance s√©mantique de topics avec synonymes
        memory_topics = set(topic.lower() for topic in memory.context_vector.topics)
        query_topics = set(topic.lower() for topic in query_vector.topics)

        # Mapping de synonymes pour topics
        topic_synonyms = {
            'cr√©ativit√©': ['cr√©ation', 'cr√©atif', 'art', 'artistique'],
            '√©motion': ['sentiment', 'ressenti', 'humeur'],
            'technique': ['technologie', 'informatique', 'programmation'],
            'relation': ['social', 'ami', 'famille'],
            'apprentissage': ['√©ducation', 'formation', '√©tude'],
            'travail': ['emploi', 'm√©tier', 'profession'],
            'voyage': ['tourisme', 'vacances', 'exploration'],
        }

        topic_score = 0
        if memory_topics and query_topics:
            for qt in query_topics:
                # Correspondance directe
                if qt in memory_topics:
                    topic_score += 1.0
                else:
                    # Correspondance par synonymes
                    for mt in memory_topics:
                        synonyms = topic_synonyms.get(qt, []) + topic_synonyms.get(mt, [])
                        if mt in synonyms or qt in synonyms:
                            topic_score += 0.8
                            break

            if topic_score > 0:
                topic_score = min(1.0, topic_score / len(query_topics))
                score += topic_score * 0.6

        # 5. Correspondance d'entit√©s avec variance
        memory_entities = set(entity.lower() for entity in memory.context_vector.entities)
        query_entities = set(entity.lower() for entity in query_vector.entities)

        if memory_entities and query_entities:
            entity_matches = len(memory_entities & query_entities)
            if entity_matches > 0:
                entity_score = entity_matches / len(query_entities)
                score += entity_score * 0.5

        # 6. Bonus pour richesse de contenu
        if len(memory.content) >= 30:
            score *= 1.1
        elif len(memory.content) < 15:
            score *= 0.8

        # 7. Bonus si tr√®s proche s√©mantiquement
        if score >= 0.7:
            score = min(1.0, score * 1.15)

        return min(1.0, max(0.0, score))

    def _calculate_emotional_relevance(self, memory: MemoryEntry, query: str) -> float:
        """Calcule la pertinence √©motionnelle"""
        if not memory.emotional_tags:
            return 0.5  # Neutre

        # D√©tection d'√©motion dans la requ√™te
        query_emotion = self._detect_query_emotion(query)

        if not query_emotion:
            # Pas d'√©motion d√©tect√©e, on valorise les m√©moires neutres
            avg_intensity = sum(tag.intensity for tag in memory.emotional_tags) / len(memory.emotional_tags)
            return 0.5 + (1.0 - avg_intensity) * 0.3  # Pr√©f√©rer les √©motions moins intenses

        # Correspondance √©motionnelle directe
        for tag in memory.emotional_tags:
            if tag.emotion == query_emotion:
                return 0.8 + tag.intensity * 0.2  # Bonus pour correspondance directe

        # Correspondance de valence (positif/n√©gatif)
        query_valence = self._get_emotion_valence(query_emotion)
        memory_valence = sum(tag.valence for tag in memory.emotional_tags) / len(memory.emotional_tags)

        if abs(query_valence - memory_valence) < 0.5:
            return 0.6  # Valence similaire

        return 0.3  # √âmotions diff√©rentes

    def _calculate_contextual_relevance(self, memory: MemoryEntry, query_vector: ContextVector) -> float:
        """Calcule la pertinence contextuelle"""
        score = 0.5  # Base

        # Bonus pour type de m√©moire pertinent
        if memory.memory_type == MemoryType.DIALOGUE:
            score += 0.2  # Les dialogues sont souvent plus pertinents
        elif memory.memory_type == MemoryType.EMOTIONAL:
            score += 0.3  # √âmotions tr√®s pertinentes

        # Bonus pour m√©moires fr√©quemment acc√©d√©es
        access_bonus = min(0.3, memory.access_count / 20.0)
        score += access_bonus

        # Bonus pour contenu riche
        content_richness = min(0.2, len(memory.content) / 500.0)
        score += content_richness

        return min(1.0, score)

    def _calculate_temporal_relevance(self, memory: MemoryEntry) -> float:
        """Calcule la pertinence temporelle optimis√©e"""
        try:
            now = datetime.now()
            memory_time = datetime.fromisoformat(memory.timestamp)
            last_access = datetime.fromisoformat(memory.last_accessed)

            # √Çge de la m√©moire
            age_days = (now - memory_time).days
            age_factor = math.exp(-age_days / 90.0)  # D√©croissance exponentielle sur 90 jours

            # R√©cence d'acc√®s
            access_days = (now - last_access).days
            recency_factor = math.exp(-access_days / 30.0)  # D√©croissance sur 30 jours

            # Facteur de decay naturel
            decay_factor = memory.decay_factor

            # Combinaison optimis√©e
            temporal_score = age_factor * 0.4 + recency_factor * 0.4 + decay_factor * 0.2

            return temporal_score

        except Exception:
            return 0.5

    def _calculate_quality_multiplier(self, memory: MemoryEntry) -> float:
        """Calcule le multiplicateur de qualit√©"""
        multiplier = 1.0

        # Bonus pour m√©moires non compress√©es (plus de d√©tails)
        if not memory.compressed:
            multiplier *= 1.1

        # Bonus pour m√©moires avec tags √©motionnels riches
        if memory.emotional_tags:
            avg_confidence = sum(tag.confidence for tag in memory.emotional_tags) / len(memory.emotional_tags)
            multiplier *= 1.0 + avg_confidence * 0.2

        # Bonus pour m√©moires avec contexte riche
        if memory.context_vector.topics or memory.context_vector.entities:
            multiplier *= 1.15

        return multiplier

    def _detect_query_emotion(self, query: str) -> str | None:
        """D√©tecte l'√©motion dans une requ√™te"""
        emotion_patterns = {
            'joie': [r'\b(heureux|joyeux|content|ravi|enchant√©|joie)\b'],
            'tristesse': [r'\b(triste|d√©prim√©|m√©lancolique|chagrin|peine)\b'],
            'col√®re': [r'\b(en col√®re|furieux|√©nerv√©|irrit√©|rage)\b'],
            'peur': [r'\b(peur|effray√©|anxieux|inquiet|angoisse)\b'],
            'surprise': [r'\b(surpris|√©tonn√©|choqu√©|stup√©fait)\b'],
            'amour': [r'\b(amour|adore|aime|affection|tendresse)\b'],
        }

        query_lower = query.lower()
        for emotion, patterns in emotion_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    return emotion

        return None

    def _get_emotion_valence(self, emotion: str) -> float:
        """Retourne la valence d'une √©motion (-1 √† 1)"""
        valences = {
            'joie': 0.8,
            'amour': 0.9,
            'surprise': 0.3,
            'tristesse': -0.7,
            'col√®re': -0.8,
            'peur': -0.6,
            'neutre': 0.0,
        }
        return valences.get(emotion, 0.0)

    def _should_promote_memory(self, memory: MemoryEntry) -> bool:
        """D√©termine si une m√©moire doit √™tre promue avec crit√®res optimis√©s"""
        if memory.memory_level == MemoryLevel.LONG_TERM:
            return False

        # Score de promotion multi-crit√®res optimis√©
        promotion_score = (
            memory.importance_score * 0.35  # Importance baseline
            + memory.access_count / 8.0 * 0.25  # Facteur d'acc√®s renforc√©
            + memory.emotional_weight * 0.25  # Poids √©motionnel
            + memory.decay_factor * 0.15  # Facteur de fra√Æcheur
        )

        # Bonus pour m√©moires avec contexte riche
        if memory.context_vector.topics or memory.context_vector.entities:
            promotion_score += 0.1

        # Bonus pour m√©moires √©motionnelles fortes
        if memory.memory_type == MemoryType.EMOTIONAL and memory.emotional_weight > 0.7:
            promotion_score += 0.15

        # Appliquer les seuils optimis√©s
        if memory.memory_level == MemoryLevel.SHORT_TERM:
            return promotion_score > self.config["promotion_thresholds"]["short_to_medium"]
        else:
            return promotion_score > self.config["promotion_thresholds"]["medium_to_long"]

    def _should_compress_memory(self, memory: MemoryEntry) -> bool:
        """D√©termine si une m√©moire doit √™tre compress√©e"""
        return (
            memory.decay_factor < self.config["compression_threshold"]
            and not memory.compressed
            and memory.memory_level != MemoryLevel.SHORT_TERM
        )

    def _should_delete_memory(self, memory: MemoryEntry) -> bool:
        """D√©termine si une m√©moire doit √™tre supprim√©e avec crit√®res optimis√©s"""
        # Crit√®res plus stricts pour √©viter la perte de m√©moires pr√©cieuses
        base_condition = (
            memory.decay_factor < 0.05  # Seuil plus strict: 5% au lieu de 10%
            and memory.access_count == 0
            and memory.importance_score < 0.2  # Seuil plus strict: 20% au lieu de 30%
        )

        # Protection absolue pour certaines m√©moires
        protected_memory = (
            memory.memory_type == MemoryType.EMOTIONAL
            or memory.memory_type == MemoryType.RELATIONSHIP
            or memory.emotional_weight > 0.5
            or len(memory.context_vector.topics) > 2
        )

        return base_condition and not protected_memory

    async def _promote_memory(self, memory: MemoryEntry):
        """Promeut une m√©moire au niveau sup√©rieur"""
        old_level = memory.memory_level

        if memory.memory_level == MemoryLevel.SHORT_TERM:
            memory.memory_level = MemoryLevel.MEDIUM_TERM
        elif memory.memory_level == MemoryLevel.MEDIUM_TERM:
            memory.memory_level = MemoryLevel.LONG_TERM

        # Mettre √† jour les index
        self.memory_index[old_level].discard(memory.id)
        self.memory_index[memory.memory_level].add(memory.id)

        logger.debug(f"üìà Memory promoted: {memory.id[:8]} -> {memory.memory_level.value}")

    async def _compress_memory(self, memory: MemoryEntry):
        """Compresse une m√©moire pour √©conomiser l'espace"""
        if memory.compressed:
            return

        # Compression simple : garder les √©l√©ments essentiels
        original_length = len(memory.content)

        # R√©sum√© du contenu (simulation)
        if len(memory.content) > 200:
            memory.content = memory.content[:100] + " [...r√©sum√©...] " + memory.content[-50:]

        memory.compressed = True
        memory.compression_ratio = len(memory.content) / original_length

        logger.debug(f"üóúÔ∏è Memory compressed: {memory.id[:8]} (ratio: {memory.compression_ratio:.2f})")

    async def _cleanup_old_memories(self):
        """Nettoie les anciennes m√©moires pour lib√©rer de l'espace"""
        # Trier par score de conservation optimis√©
        memories_by_score = []
        for memory in self.memories.values():
            conservation_score = (
                memory.importance_score * 0.35  # Importance augment√©e
                + memory.decay_factor * 0.25  # Facteur de decay
                + memory.access_count / 8.0 * 0.25  # Acc√®s fr√©quents valoris√©s
                + memory.emotional_weight * 0.15  # Poids √©motionnel
            )
            memories_by_score.append((conservation_score, memory))

        memories_by_score.sort(key=lambda x: x[0])

        # Supprimer seulement 5% de m√©moires (au lieu de 10%) pour √©viter pertes importantes
        to_delete = max(1, int(len(memories_by_score) * 0.05))
        deleted_count = 0

        for _, memory in memories_by_score:
            if deleted_count >= to_delete:
                break

            # Protection contre suppression de m√©moires pr√©cieuses
            if not self._is_protected_memory(memory):
                await self.delete_memory(memory.id)
                deleted_count += 1

        logger.info(f"üßπ Cleaned up {deleted_count} old memories")

    async def _batch_cleanup_memories(self):
        """Nettoyage en batch pour de meilleures performances"""
        batch_size = self.config.get("batch_size", 100)
        memories_to_process = list(self.memories.values())

        # Traiter par batch
        for i in range(0, len(memories_to_process), batch_size):
            batch = memories_to_process[i : i + batch_size]
            await self._process_memory_batch(batch)

        # Cleanup final
        await self._cleanup_old_memories()

    async def _process_memory_batch(self, memories: list[MemoryEntry]):
        """Traite un batch de m√©moires pour optimisation"""
        # Decay en batch
        current_time = datetime.now()

        for memory in memories:
            # Application rapide du decay
            memory_age = current_time - datetime.fromisoformat(memory.timestamp)
            decay_rate = self.config["decay_rates"][memory.memory_level.value]
            age_factor = memory_age.days * decay_rate * 0.1  # R√©duction du decay

            # Protection √©motionnelle
            emotional_protection = memory.emotional_weight * 0.2

            # Application du decay
            memory.decay_factor = max(0.0, memory.decay_factor - age_factor + emotional_protection)

    def _is_protected_memory(self, memory: MemoryEntry) -> bool:
        """D√©termine si une m√©moire est prot√©g√©e contre la suppression"""
        return (
            memory.memory_type in [MemoryType.EMOTIONAL, MemoryType.RELATIONSHIP]
            or memory.emotional_weight > 0.6
            or memory.importance_score > 0.8
            or memory.access_count > 5
            or len(memory.context_vector.topics) > 2
            or memory.memory_level == MemoryLevel.LONG_TERM
        )

    # üß† JEFFREY V2.0 DYNAMIC LEARNING SYSTEM INTEGRATION

    async def store_learned_knowledge(self, knowledge_entry: dict[str, Any]) -> str:
        """
        Store learned knowledge as a special memory entry

        Args:
            knowledge_entry: Learned knowledge from ImmediateLearningManager

        Returns:
            Memory ID for the stored knowledge
        """
        try:
            # Create content description for the learned knowledge
            knowledge_type = knowledge_entry.get('type', 'unknown')
            knowledge_value = knowledge_entry.get('value', '')
            source = knowledge_entry.get('source', 'external_ai')

            content = f"Learned {knowledge_type}: {knowledge_value}"

            # Store as relationship memory with high importance
            memory_id = await self.store_memory(
                content=content,
                memory_type=MemoryType.RELATIONSHIP,
                user_id="learning_system",
                conversation_id="dynamic_learning",
                metadata={
                    'is_learned_knowledge': True,
                    'knowledge_type': knowledge_type,
                    'knowledge_value': knowledge_value,
                    'learning_source': source,
                    'learned_at': knowledge_entry.get('learned_at'),
                    'confidence': knowledge_entry.get('confidence', 0.8),
                    'context': knowledge_entry.get('context', {}),
                },
            )

            # Mark as protected and important
            if memory_id in self.memories:
                memory = self.memories[memory_id]
                memory.importance_score = 0.9  # High importance
                memory.memory_level = MemoryLevel.LONG_TERM  # Long-term storage
                memory.emotional_weight = 0.7  # Moderate emotional weight

            logger.info(f"üß† Stored learned knowledge: {knowledge_type} = {knowledge_value}")
            return memory_id

        except Exception as e:
            logger.error(f"Error storing learned knowledge: {e}")
            return ""

    async def update_user_profile(self, field: str, value: str, user_id: str = "default") -> bool:
        """
        Update user profile information in memory

        Args:
            field: Profile field to update (name, location, job, etc.)
            value: New value for the field
            user_id: User identifier

        Returns:
            Success status
        """
        try:
            content = f"User {field}: {value}"

            # Check if similar profile info already exists
            existing_memories = await self.recall_contextual(query=f"user {field}", user_id=user_id, limit=5)

            # Update existing or create new
            updated = False
            for memory in existing_memories:
                if memory.metadata and memory.metadata.get('profile_field') == field:
                    # Update existing memory
                    await self.update_memory(
                        memory.id,
                        {
                            'content': content,
                            'metadata': {
                                **memory.metadata,
                                'profile_value': value,
                                'last_updated': datetime.now().isoformat(),
                            },
                        },
                    )
                    updated = True
                    break

            if not updated:
                # Create new profile memory
                await self.store_memory(
                    content=content,
                    memory_type=MemoryType.RELATIONSHIP,
                    user_id=user_id,
                    metadata={
                        'is_user_profile': True,
                        'profile_field': field,
                        'profile_value': value,
                        'created_at': datetime.now().isoformat(),
                    },
                )

            logger.info(f"üë§ Updated user profile: {field} = {value}")
            return True

        except Exception as e:
            logger.error(f"Error updating user profile: {e}")
            return False

    async def update_jeffrey_profile(self, field: str, value: str) -> bool:
        """
        Update Jeffrey's self-knowledge in memory

        Args:
            field: Profile field to update
            value: New value for the field

        Returns:
            Success status
        """
        try:
            content = f"Jeffrey {field}: {value}"

            # Store as Jeffrey's self-knowledge
            await self.store_memory(
                content=content,
                memory_type=MemoryType.RELATIONSHIP,
                user_id="jeffrey_self",
                metadata={
                    'is_jeffrey_profile': True,
                    'profile_field': field,
                    'profile_value': value,
                    'created_at': datetime.now().isoformat(),
                },
            )

            logger.info(f"ü§ñ Updated Jeffrey profile: {field} = {value}")
            return True

        except Exception as e:
            logger.error(f"Error updating Jeffrey profile: {e}")
            return False

    async def get_learned_knowledge(self, knowledge_type: str = None, user_id: str = "default") -> list[dict[str, Any]]:
        """
        Retrieve learned knowledge from memory

        Args:
            knowledge_type: Optional filter by knowledge type
            user_id: User identifier

        Returns:
            List of learned knowledge entries
        """
        try:
            learned_knowledge = []

            for memory in self.memories.values():
                if (
                    memory.metadata
                    and memory.metadata.get('is_learned_knowledge')
                    and (not knowledge_type or memory.metadata.get('knowledge_type') == knowledge_type)
                ):
                    learned_knowledge.append(
                        {
                            'type': memory.metadata.get('knowledge_type'),
                            'value': memory.metadata.get('knowledge_value'),
                            'source': memory.metadata.get('learning_source'),
                            'learned_at': memory.metadata.get('learned_at'),
                            'confidence': memory.metadata.get('confidence', 0.8),
                            'memory_id': memory.id,
                        }
                    )

            return learned_knowledge

        except Exception as e:
            logger.error(f"Error getting learned knowledge: {e}")
            return []

    async def mark_as_known(self, knowledge_type: str, value: str) -> bool:
        """
        Mark specific knowledge as known/verified

        Args:
            knowledge_type: Type of knowledge
            value: Knowledge value

        Returns:
            Success status
        """
        try:
            # Find the relevant memory and mark as verified
            for memory in self.memories.values():
                if (
                    memory.metadata
                    and memory.metadata.get('is_learned_knowledge')
                    and memory.metadata.get('knowledge_type') == knowledge_type
                    and memory.metadata.get('knowledge_value') == value
                ):
                    # Update metadata to mark as verified
                    await self.update_memory(
                        memory.id,
                        {'metadata': {**memory.metadata, 'verified': True, 'verified_at': datetime.now().isoformat()}},
                    )

                    # Boost importance
                    memory.importance_score = min(1.0, memory.importance_score + 0.1)
                    logger.info(f"‚úÖ Marked as known: {knowledge_type} = {value}")
                    return True

            return False

        except Exception as e:
            logger.error(f"Error marking as known: {e}")
            return False


# Instance globale pour faciliter l'int√©gration
_memory_core_instance: MemoryCore | None = None


def get_memory_core() -> MemoryCore:
    """R√©cup√®re ou cr√©e l'instance globale de MemoryCore"""
    global _memory_core_instance
    if _memory_core_instance is None:
        _memory_core_instance = MemoryCore()
    return _memory_core_instance


# Fonction de compatibilit√© pour l'int√©gration
def initialize_memory_systems(config_path: str = "data/memory_config.json") -> MemoryCore:
    """Initialise le syst√®me de m√©moire avec configuration"""
    global _memory_core_instance
    _memory_core_instance = MemoryCore(config_path)
    return _memory_core_instance
