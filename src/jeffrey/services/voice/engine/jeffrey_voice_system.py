"""
Module de service syst√®me sp√©cialis√© pour Jeffrey OS.

Ce module impl√©mente les fonctionnalit√©s essentielles pour module de service syst√®me sp√©cialis√© pour jeffrey os.
Il fournit une architecture robuste et √©volutive int√©grant les composants
n√©cessaires au fonctionnement optimal du syst√®me. L'impl√©mentation suit
les principes de modularit√© et d'extensibilit√© pour faciliter l'√©volution
future du syst√®me.

Le module g√®re l'initialisation, la configuration, le traitement des donn√©es,
la communication inter-composants, et la persistance des √©tats. Il s'int√®gre
harmonieusement avec l'architecture globale de Jeffrey OS tout en maintenant
une s√©paration claire des responsabilit√©s.

L'architecture interne permet une √©volution adaptative bas√©e sur les interactions
et l'apprentissage continu, contribuant √† l'√©mergence d'une conscience artificielle
coh√©rente et authentique.
"""

from __future__ import annotations

import logging
import random
import time
from collections.abc import Callable
from pathlib import Path
from typing import Any

from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()
logging.info("‚úÖ Fichier .env charg√© correctement dans jeffrey_voice_system.py")


class JeffreyVoiceSystem:
    """
    Classe JeffreyVoiceSystem pour le syst√®me Jeffrey OS.

    Cette classe impl√©mente les fonctionnalit√©s sp√©cifiques n√©cessaires
    au bon fonctionnement du module. Elle g√®re l'√©tat interne, les transformations
    de donn√©es, et l'interaction avec les autres composants du syst√®me.
    """

    def __init__(self, config=None, emotional_core=None, test_mode=False) -> None:
        import logging

        self.config = config or {}
        self.emotional_core = emotional_core
        self.test_mode = test_mode
        self.empathic_inversion_module = None
        self.tactile_gesture_responder = None
        self.logger = logging.getLogger("JeffreyVoiceSystem")
        self.voice_engine = None
        self.adaptive_personality_engine = None  # Module d'adaptation de personnalit√©
        self.voice_mode = "ON"  # Par d√©faut, la voix est activ√©e
        # ATTENTION : NE PAS MODIFIER CES VALEURS
        self.backend = "elevenlabs"  # Backend par d√©faut - TOUJOURS ELEVENLABS
        self.use_mock_voice = False  # OBLIGATOIRE: Force l'utilisation de la vraie voix
        self.volume = 1.0  # Volume par d√©faut
        self.cache_enabled = True  # Cache activ√© par d√©faut

        # Chemin vers le dossier des effets audio g√©n√©r√©s
        self.effects_dir = Path("offline_voice_cache/effects")

        # Initialisation des effets audio
        self.audio_fx = {
            "soupir_doux": "assets/audio_fx/soupir_doux.wav",
            "respiration_lente": "assets/audio_fx/respiration_lente.wav",
            "pause_√©mue": "assets/audio_fx/pause_√©mue.wav",
            "soupir_profond": "assets/audio_fx/soupir_profond.wav",
            "respiration_√©mue": "assets/audio_fx/respiration_√©mue.wav",
        }

        # V√©rifier la disponibilit√© des modules audio
        self.audio_modules_available = False
        try:
            pass

            self.audio_modules_available = True
            self.logger.info("‚úÖ Modules audio (sounddevice, soundfile) disponibles")
        except ImportError:
            self.logger.warning("‚ö†Ô∏è Modules audio non disponibles. Les effets sonores seront d√©sactiv√©s.")
            print("‚ö†Ô∏è Pour activer les effets sonores, installez : pip install sounddevice soundfile")

        # Variables pour la reconnaissance vocale
        self.speech_recognizer = None
        self.speech_recognition_thread = None
        self.listening_active = False
        self.wake_word = "Jeffrey"
        self.continuous_listening = True
        self.sensitivity = 0.7
        self.recognition_callback = None
        self.recognition_language = "fr-FR"

        if not self.test_mode:
            self.initialize_components()

    def initialize_components(self):
        """Initialize the voice system components."""


import logging

try:
    logging.info("[JeffreyVoiceSystem] Initialisation du syst√®me vocal")
    # Tentative de chargement du moteur de voix r√©el
    try:
        from orchestrateur.core.voice.voice_engine import VoiceEngine

        self.voice_engine = VoiceEngine()
        logging.info("[JeffreyVoiceSystem] Moteur vocal principal charg√© avec succ√®s")
    except ImportError:
        logging.warning(
            "[JeffreyVoiceSystem] Impossible de charger le moteur vocal principal, utilisation du mode de secours"
        )
        self.voice_engine = None

    # Affichage de l'√©tat du syst√®me vocal au d√©marrage
    logging.info(
        f"[JeffreyVoiceSystem] √âtat au d√©marrage: voice_mode={self.voice_mode}, backend={self.backend}, cache_enabled={self.cache_enabled}, volume={self.volume}"
    )
    print(
        f"üîä Syst√®me vocal: voice_mode={self.voice_mode}, backend={self.backend}, cache_enabled={self.cache_enabled}, volume={self.volume}"
    )
except Exception as e:
    logging.error(f"[JeffreyVoiceSystem] Erreur lors de l'initialisation: {e}")


def initialize(self):
    """Initialize the voice system - compatibility method."""
    # Compatibility method that calls initialize_components
    self.initialize_components()


def shutdown(self):
    """Ferme proprement le syst√®me vocal."""
    import logging

    logging.info("[JeffreyVoiceSystem] Fermeture du syst√®me vocal")

    # Fermer le moteur vocal si disponible
    if self.voice_engine and hasattr(self.voice_engine, "shutdown"):
        try:
            self.voice_engine.shutdown()
            logging.info("[JeffreyVoiceSystem] Moteur vocal ferm√© avec succ√®s")
        except Exception as e:
            logging.error(f"[JeffreyVoiceSystem] Erreur lors de la fermeture du moteur vocal: {e}")

    # Fermer les ressources audio
    try:
        # Tentative de fermeture de pygame si utilis√©
        try:
            import pygame

            if pygame.mixer.get_init():
                pygame.mixer.quit()
                logging.info("[JeffreyVoiceSystem] Ressources audio pygame lib√©r√©es")
        except BaseException:
            pass
    except Exception as e:
        logging.error(f"[JeffreyVoiceSystem] Erreur lors de la lib√©ration des ressources audio: {e}")

    logging.info("[JeffreyVoiceSystem] Syst√®me vocal ferm√©")


def _ajouter_effets_vocaux(self, texte: str, emotion: str = "neutre") -> str:
    """
    Ajoute des effets vocaux textuels pour enrichir l'expression √©motionnelle.

    Args:
        texte: Le texte √† enrichir
        emotion: L'√©motion actuelle

    Returns:
        str: Le texte enrichi avec des effets vocaux
    """
    import random

    # Effets vocaux selon l'√©motion
    effets_emotionnels = {
        "tendresse": {
            "soupirs": ["*soupir doux*", "*respire doucement*", "*silence intime*"],
            "intros": [
                "Je...",
                "Tu sais...",
                "C'est difficile √† dire... mais",
                "J'ai besoin de le dire...",
                "Je ressens que...",
                "Mon c≈ìur me dit que...",
            ],
        },
        "amour": {
            "soupirs": ["*soupir profond*", "*respire avec √©motion*", "*silence √©mu*"],
            "intros": [
                "Je t'aime tellement...",
                "Mon c≈ìur bat si fort...",
                "Je ressens tant d'amour...",
                "Je ne peux m'emp√™cher de...",
                "Je suis si heureux quand...",
            ],
        },
        "tristesse": {
            "soupirs": ["*soupir m√©lancolique*", "*respire lentement*", "*silence triste*"],
            "intros": [
                "Je me sens...",
                "Parfois je...",
                "C'est dur de...",
                "J'ai l'impression que...",
                "Je ne sais pas pourquoi mais...",
            ],
        },
        "doute": {
            "soupirs": ["*soupir h√©sitant*", "*respire nerveusement*", "*silence pensif*"],
            "intros": [
                "Je me demande si...",
                "Peut-√™tre que...",
                "Je ne suis pas s√ªr mais...",
                "J'h√©site √† dire que...",
                "Je pense que...",
            ],
        },
    }

    # Effets par d√©faut pour les autres √©motions
    effets_par_defaut = {
        "soupirs": ["*soupir*", "*respire*", "*silence*"],
        "intros": ["Je...", "Tu sais...", "C'est que...", "Je pense que...", "Je crois que..."],
    }

    # S√©lectionner les effets selon l'√©motion
    effets = effets_emotionnels.get(emotion, effets_par_defaut)

    # Ajouter un soupir au d√©but avec une probabilit√© de 40%
    if random.random() < 0.4:
        texte = f"{random.choice(effets['soupirs'])}\n\n{texte}"

    # Ajouter une introduction √©motionnelle avec une probabilit√© de 50%
    if random.random() < 0.5:
        phrases = texte.split(". ")
    if phrases:
        # Choisir une phrase au hasard pour l'intro
        idx = random.randint(0, len(phrases) - 1)
        phrases[idx] = f"{random.choice(effets['intros'])} {phrases[idx]}"
        texte = ". ".join(phrases)

    # Ajouter des pauses √©motionnelles avec une probabilit√© de 30%
    if random.random() < 0.3:
        texte = texte.replace(". ", "... ").replace("! ", "...! ").replace("? ", "...? ")

    return texte


def play_audio_effect(self, effect_key: str, volume: float = 1.0) -> bool:
    """
    Joue un effet audio g√©n√©r√© par ElevenLabs depuis le cache offline.

    Args:
        effect_key: Cl√© de l'effet √† jouer (ex: "soupir_doux")
        volume: Volume de l'effet (0.0 √† 1.0)

    Returns:
        bool: True si l'effet a √©t√© jou√© avec succ√®s
    """
    if not self.audio_modules_available:
        self.logger.debug(f"Modules audio non disponibles, effet '{effect_key}' ignor√©")
    return False

    import sounddevice as sd
    import soundfile as sf

    # Construire le chemin du fichier
    effect_path = self.effects_dir / f"{effect_key}.wav"

    # V√©rifier si le fichier existe
    if not effect_path.exists():
        self.logger.warning(f"Effet audio non trouv√© : {effect_path}")
    return False

    try:
        # Lire le fichier audio
        data, samplerate = sf.read(str(effect_path))

        # Ajuster le volume
        if volume != 1.0:
            data = data * volume

        # Jouer l'audio
        self.logger.info(f"üéß Lecture effet audio : {effect_key}.wav")
        sd.play(data, samplerate)
        sd.wait()  # Attendre la fin de la lecture

        return True

    except Exception as e:
        self.logger.error(f"Erreur lors de la lecture de l'effet {effect_key}: {e}")
        return False


def speak(
    self,
    text: str,
    emotion: str = "neutre",
    phase: str = "adulte",
    play_audio: bool = True,
    emphasis: float = 0.7,
    slow_mode: bool = False,
    effect_start: str | None = None,
    effect_end: str | None = None,
    **kwargs,
) -> str | None:
    """
    Fait parler Jeffrey avec des effets audio √©motionnels.

    Args:
        text: Le texte √† prononcer
        emotion: L'√©motion √† exprimer
        phase: La phase de d√©veloppement
        play_audio: Indique s'il faut jouer l'audio
        emphasis: L'intensit√© de l'√©motion (0.0 √† 1.0)
        slow_mode: Mode lent pour les sc√®nes intimes
        effect_start: Effet audio sp√©cifique au d√©but (optionnel)
        effect_end: Effet audio sp√©cifique √† la fin (optionnel)
        **kwargs: Param√®tres suppl√©mentaires

    Returns:
        str: Identifiant de l'audio g√©n√©r√© ou None
    """
    import logging
    import random

    # V√©rifier si la voix est activ√©e
    if self.voice_mode.upper() == "OFF":
        logging.info(f"[JeffreyVoiceSystem] Voix d√©sactiv√©e. Message non prononc√©: {text[:50]}...")
    return None

    # D√©terminer les effets audio √† jouer
    should_play_effects = slow_mode or emotion in ["tendresse", "amour", "tristesse", "doute"]

    if should_play_effects and self.audio_modules_available:
        # Effet au d√©but
        if effect_start:
            start_effect = effect_start
        else:
            # S√©lectionner l'effet selon l'√©motion
            start_effects = {
                "tendresse": "soupir_doux",
                "amour": "soupir_profond",
                "tristesse": "respiration_lente",
                "doute": "pause_√©mue",
            }
            start_effect = start_effects.get(emotion, "respiration_lente")

        # Jouer l'effet de d√©but
    if self.play_audio_effect(start_effect, volume=0.8):
        self.logger.info(f"üí® Effet audio d√©but: {start_effect}.wav")
        # TODO: Remplacer par asyncio.sleep ou threading.Event  # Pause naturelle apr√®s l'effet

    # Enrichir le texte avec des effets textuels
    text = self._ajouter_effets_vocaux(text, emotion)

    logging.info(
        f"[JeffreyVoiceSystem] üîä G√©n√©ration vocale: {text[:50]}... [√©motion: {emotion}, phase: {phase}, volume: {self.volume}, slow_mode: {slow_mode}]"
    )
    print(f"üé§ Jeffrey dit ({emotion}, {phase}, slow_mode={slow_mode}): {text[:50]}...")

    # Adapter les param√®tres vocaux pour le mode lent
    if slow_mode:
        kwargs["tempo"] = kwargs.get("tempo", 0.8)
        kwargs["pitch"] = kwargs.get("pitch", 0.9)
        emphasis = min(emphasis * 1.2, 1.0)

    # G√©n√©rer la voix
    result = None
    if self.voice_engine and hasattr(self.voice_engine, "say"):
        try:
            result = self.voice_engine.say(
                text=text,
                emotion=emotion,
                phase=phase,
                play_audio=play_audio,
                emphasis=emphasis,
                volume=self.volume,
                cache_enabled=self.cache_enabled,
                **kwargs,
            )
        except Exception as e:
            logging.error(f"Erreur lors de la g√©n√©ration vocale: {e}")
            result = None

    # Effet √† la fin (30-40% de chance)
    if should_play_effects and self.audio_modules_available and random.random() < 0.35:
        # TODO: Remplacer par asyncio.sleep ou threading.Event  # Petite pause avant l'effet final

        if effect_end:
            end_effect = effect_end
        else:
            # S√©lectionner un effet final diff√©rent
            end_effects = {
                "tendresse": "respiration_√©mue",
                "amour": "murmure_je_taime",
                "tristesse": "pause_√©mue",
                "doute": "h√©sitation_amour",
            }
            end_effect = end_effects.get(emotion, "pause_√©mue")

        if self.play_audio_effect(end_effect, volume=0.7):
            self.logger.info(f"üí® Effet audio fin: {end_effect}.wav")

    return result


def play_sound_auto(self, sound_type, emotion=None):
    """
    Joue un son en fonction du type et de l'√©motion.

    Args:
        sound_type: Type de son √† jouer
        emotion: √âmotion associ√©e au son

    Returns:
        bool: True si le son a √©t√© jou√©, False sinon
    """
    import logging

    # V√©rifier si la voix est activ√©e
    if self.voice_mode.upper() == "OFF":
        logging.info(f"[JeffreyVoiceSystem] Voix d√©sactiv√©e. Son non jou√©: {sound_type}")
    return False

    logging.info(f"[JeffreyVoiceSystem] üîä Lecture du son: {sound_type} [√©motion: {emotion}]")

    # Si le moteur de voix est disponible et a une m√©thode pour jouer des sons
    if self.voice_engine and hasattr(self.voice_engine, "play_sound"):
        try:
            return self.voice_engine.play_sound(sound_type, emotion=emotion)
        except Exception as e:
            logging.error(f"[JeffreyVoiceSystem] Erreur lors de la lecture du son: {e}")

    # Simule le son en console
    print(f"[SOUND] {sound_type} ({emotion})")
    return True


def set_adaptive_personality_engine(self, engine) -> None:
    """
    D√©finit le moteur d'adaptation de personnalit√© √† utiliser.

    Args:
        engine: Instance de AdaptivePersonalityEngine
    """
    self.adaptive_personality_engine = engine
    self.logger.info("Moteur d'adaptation de personnalit√© connect√© au syst√®me vocal")


def apply_empathic_inversion(self, response: str, user_emotional_state: dict[str, Any]) -> tuple[str, dict[str, Any]]:
    """
    Applique une inversion empathique √† une r√©ponse.
    Impl√©ment√© pour le Sprint 201.

    Args:
        response: R√©ponse textuelle originale
        user_emotional_state: √âtat √©motionnel de l'utilisateur

    Returns:
        Tuple[str, Dict]: R√©ponse modifi√©e et informations d'inversion
    """
    # V√©rifier si le module d'inversion empathique est disponible
    if not self.empathic_inversion_module:
        return response, {"success": False, "reason": "EmpathicInversionModule non initialis√©"}

    try:
        # Traiter l'√©tat √©motionnel de l'utilisateur
        inversion_info = self.empathic_inversion_module.process_user_emotional_state(user_emotional_state)

        # Si une inversion est n√©cessaire, l'appliquer √† la r√©ponse
        if inversion_info.get("inversion_needed", False) and inversion_info.get("inversion_applied", False):
            modified_response, voice_params = self.empathic_inversion_module.apply_empathic_inversion_to_response(
                response, inversion_info
            )

            result = {
                "success": True,
                "inversion_applied": True,
                "original_emotion": inversion_info.get("original_emotion"),
                "inverse_emotion": inversion_info.get("inverse_emotion"),
                "voice_params": voice_params,
            }

            # Si le module est int√©gr√© au c≈ìur √©motionnel, enregistrer √©galement l√†-bas
            if self.emotional_core and hasattr(self.emotional_core, "appliquer_inversion_empathique"):
                _, emotional_result = self.emotional_core.appliquer_inversion_empathique(response, user_emotional_state)
                # Fusionner les r√©sultats si n√©cessaire
                if emotional_result.get("success", False):
                    self.logger.info("Inversion empathique √©galement trait√©e par le c≈ìur √©motionnel")

            return modified_response, result
        else:
            return response, {
                "success": True,
                "inversion_applied": False,
                "reason": inversion_info.get("reason", "Inversion non n√©cessaire"),
            }

    except Exception as e:
        self.logger.error(f"Erreur lors de l'application de l'inversion empathique: {e}")
        return response, {"success": False, "reason": str(e)}


def process_tactile_gesture(
    self,
    zone: str,
    gesture_type: str,
    intensity: float = 0.5,
    duration: float = 1.0,
    user_id: str | None = None,
) -> dict[str, Any]:
    """
    Traite un geste tactile provenant de l'interface utilisateur.
    Impl√©ment√© pour le Sprint 202.

    Args:
        zone: Zone touch√©e
        gesture_type: Type de geste
        intensity: Intensit√© du geste (0-1)
        duration: Dur√©e du geste en secondes
        user_id: Identifiant de l'utilisateur (optionnel)

    Returns:
        Dict: R√©sultat du traitement
    """
    # Si le c≈ìur √©motionnel a un tactile_gesture_responder, utiliser celui-l√†
    if self.emotional_core and hasattr(self.emotional_core, "traiter_geste_tactile"):
        try:
            return self.emotional_core.traiter_geste_tactile(
                zone=zone,
                type_toucher=gesture_type,
                intensite=intensity,
                duration=duration,
                user_id=user_id,
            )
        except Exception as e:
            self.logger.error(f"Erreur lors du traitement du geste tactile via le c≈ìur √©motionnel: {e}")

    # Sinon, utiliser le module local s'il est disponible
    if not self.tactile_gesture_responder:
        return {"success": False, "reason": "TactileGestureResponder non disponible"}

    try:
        # Traiter le geste tactile
        result = self.tactile_gesture_responder.process_gesture(
            zone=zone,
            gesture_type=gesture_type,
            intensity=intensity,
            duration=duration,
            user_id=user_id,
            source="voice_system",
        )

        # Enrichir le r√©sultat
        result["success"] = result.get("success", True)

        return result

    except Exception as e:
        self.logger.error(f"Erreur lors du traitement du geste tactile: {e}")
        return {"success": False, "reason": str(e)}


def interjection(
    self,
    type_interjection: str = "neutre",
    texte: str | None = None,
    params_vocaux: dict[str, Any] | None = None,
) -> str:
    """
    Produit une interjection vocale selon un type d'√©motion.
    Utile pour le TactileGestureResponder (Sprint 202).

    Args:
        type_interjection: Type d'interjection ("surprise", "toucher", etc.)
        texte: Texte sp√©cifique de l'interjection (optionnel)
        params_vocaux: Param√®tres vocaux sp√©cifiques (optionnel)

    Returns:
        str: Chemin du fichier audio g√©n√©r√©, ou None en cas d'√©chec
    """
    # Mapper les types d'interjection vers des √©motions
    emotion_map = {
        "surprise": "surprise",
        "toucher": "plaisir",
        "affectueux": "affection",
        "stimulant": "joie",
        "intense": "alerte",
        "neutre": "int√©r√™t",
    }

    # D√©terminer le texte de l'interjection si non sp√©cifi√©
    if not texte:
        interjections = {
            "surprise": ["Oh!", "Ah!", "Wow!"],
            "toucher": ["Mmh", "Oh"],
            "affectueux": ["Mmmm...", "Aaah"],
            "stimulant": ["Hihihi", "Oh l√† l√†!"],
            "intense": ["Oh!", "A√Øe", "H√©!"],
            "neutre": ["Hm", "Oh"],
        }
        texte = random.choice(interjections.get(type_interjection, ["Oh"]))

    # D√©terminer l'√©motion √† utiliser
    emotion = emotion_map.get(type_interjection, "neutre")

    # Pr√©parer les param√®tres vocaux
    voice_params = params_vocaux or {}
    if "emphasis" not in voice_params:
        voice_params["emphasis"] = 0.7  # Intensit√© par d√©faut

    # G√©n√©rer la voix avec des param√®tres sp√©cifiques
    try:
        # Utiliser directement l'engine si possible
        if hasattr(self, "voice_engine") and hasattr(self.voice_engine, "say"):
            return self.voice_engine.say(
                text=texte,
                emotion=emotion,
                emphasis=voice_params.get("emphasis", 0.7),
                output_path=None,
                play_audio=True,
            )
        # Sinon, utiliser la m√©thode g√©n√©rique
        elif hasattr(self, "dire") or hasattr(self, "parler"):
            method = getattr(self, "dire", None) or getattr(self, "parler", None)
            return method(texte, emotion=emotion, voice_params=voice_params)

        return None

    except Exception as e:
        self.logger.error(f"Erreur lors de la g√©n√©ration de l'interjection: {e}")
        return None


def soupirer(self, type_soupir: str = "neutre", intensite: float = 0.7) -> str:
    """
    Produit un soupir vocal selon un type d'√©motion.
    Utile pour le TactileGestureResponder (Sprint 202).

    Args:
        type_soupir: Type de soupir ("plaisir", "fatigue", etc.)
        intensite: Intensit√© du soupir (0-1)

    Returns:
        str: Chemin du fichier audio g√©n√©r√©, ou None en cas d'√©chec
    """
    # Mapper les types de soupir vers des textes et √©motions
    soupir_map = {
        "plaisir": {"texte": "Mmmmh...", "emotion": "plaisir"},
        "fatigue": {"texte": "Pfff...", "emotion": "fatigue"},
        "soulagement": {"texte": "Aaah...", "emotion": "soulagement"},
        "inqui√©tude": {"texte": "Hmmm...", "emotion": "inqui√©tude"},
        "sensible": {"texte": "Aah...", "emotion": "plaisir"},
        "neutre": {"texte": "Hmm...", "emotion": "neutre"},
    }

    # R√©cup√©rer les infos du soupir
    soupir_info = soupir_map.get(type_soupir, soupir_map["neutre"])

    # Param√®tres vocaux pour un effet de soupir
    params_vocaux = {
        "emphasis": intensite,
        "breathiness": 0.8,  # Tr√®s souffl√© pour un soupir
        # Note: rate and pitch parameters not supported by ElevenLabs API. Using only supported API parameters.
    }

    # G√©n√©rer l'interjection
    return self.interjection(type_interjection=type_soupir, texte=soupir_info["texte"], params_vocaux=params_vocaux)


# SPRINT 218: Int√©gration du module de stabilit√© √©motionnelle


def apply_emotional_stability_to_voice(self, voice_params: dict[str, Any], current_emotion: str) -> dict[str, Any]:
    """
    Applique le verrou de stabilit√© √©motionnelle aux param√®tres vocaux si n√©cessaire.
    Impl√©ment√© pour le Sprint 218.

    Args:
        voice_params: Param√®tres vocaux √† adapter
        current_emotion: √âmotion actuelle demand√©e

    Returns:
        Dict[str, Any]: Param√®tres vocaux adapt√©s
    """
    # Si le c≈ìur √©motionnel n'a pas de module de stabilit√©, retourner les param√®tres tels quels
    if not self.emotional_core or not hasattr(self.emotional_core, "obtenir_etat_verrou_stabilite"):
        return voice_params

    try:
        # V√©rifier si un verrou de stabilit√© est actif
        stability_status = self.emotional_core.obtenir_etat_verrou_stabilite()

        if not stability_status.get("success", False) or not stability_status.get("verrou_actif", False):
            return voice_params  # Pas de verrou actif, aucune modification n√©cessaire

        # R√©cup√©rer le niveau de verrou et l'√©tat √©motionnel stable
        lock_level = stability_status.get("niveau_verrou", "mod√©r√©")
        stable_emotion = stability_status.get("emotion_stable", "neutre")

        # Si l'√©motion demand√©e est la m√™me que l'√©motion stable, pas besoin d'adapter
        if current_emotion == stable_emotion:
            return voice_params

        # Adapter les param√®tres vocaux selon le niveau de verrou
        adapted_params = voice_params.copy()

        if lock_level == "l√©ger":
            # L√©ger: r√©duire l√©g√®rement l'intensit√© de l'√©motion
            if "emphasis" in adapted_params:
                adapted_params["emphasis"] = max(0.3, adapted_params.get("emphasis", 0.5) * 0.7)
            if "emotion_intensity" in adapted_params:
                adapted_params["emotion_intensity"] = max(0.3, adapted_params.get("emotion_intensity", 0.5) * 0.7)

        elif lock_level == "mod√©r√©":
            # Mod√©r√©: m√©langer l'√©motion demand√©e avec l'√©motion stable
            if "emphasis" in adapted_params:
                adapted_params["emphasis"] = max(0.2, adapted_params.get("emphasis", 0.5) * 0.5)
            if "emotion_intensity" in adapted_params:
                adapted_params["emotion_intensity"] = max(0.2, adapted_params.get("emotion_intensity", 0.5) * 0.5)

            # Ajouter l'√©motion stable comme √©motion secondaire
            if "secondary_emotion" not in adapted_params:
                adapted_params["secondary_emotion"] = stable_emotion
                adapted_params["secondary_emotion_intensity"] = 0.4

        elif lock_level == "strict":
            # Strict: remplacer l'√©motion par l'√©motion stable
            adapted_params["emotion"] = stable_emotion
            if "emphasis" in adapted_params:
                adapted_params["emphasis"] = min(0.6, adapted_params.get("emphasis", 0.5))
            if "emotion_intensity" in adapted_params:
                adapted_params["emotion_intensity"] = min(0.6, adapted_params.get("emotion_intensity", 0.5))

            # Supprimer toute √©motion secondaire
            if "secondary_emotion" in adapted_params:
                del adapted_params["secondary_emotion"]
            if "secondary_emotion_intensity" in adapted_params:
                del adapted_params["secondary_emotion_intensity"]

        self.logger.info(f"Param√®tres vocaux adapt√©s pour respecter le verrou de stabilit√© (niveau: {lock_level})")
        return adapted_params

    except Exception as e:
        self.logger.error(f"Erreur lors de l'application de la stabilit√© √©motionnelle aux param√®tres vocaux: {e}")
    return voice_params  # En cas d'erreur, retourner les param√®tres originaux


# SPRINT 219: Int√©gration du m√©tamoniteur √©motionnel


def adjust_voice_for_emotional_patterns(self, voice_params: dict[str, Any], current_emotion: str) -> dict[str, Any]:
    """
    Ajuste les param√®tres vocaux en fonction des patterns √©motionnels d√©tect√©s.
    Impl√©ment√© pour le Sprint 219.

    Args:
        voice_params: Param√®tres vocaux √† adapter
        current_emotion: √âmotion actuelle demand√©e

    Returns:
        Dict[str, Any]: Param√®tres vocaux adapt√©s
    """
    # Si le c≈ìur √©motionnel n'a pas de m√©tamoniteur, retourner les param√®tres tels quels
    if not self.emotional_core or not hasattr(self.emotional_core, "obtenir_patterns_emotionnels"):
        return voice_params

    try:
        # R√©cup√©rer les patterns √©motionnels actuels
        patterns_result = self.emotional_core.obtenir_patterns_emotionnels()

        if not patterns_result.get("success", False):
            return voice_params  # Pas de patterns disponibles, aucune modification n√©cessaire

        patterns = patterns_result.get("patterns", {})

        # Si aucun pattern probl√©matique n'est d√©tect√©, pas besoin d'adapter
        problematic_patterns = [p for p, v in patterns.items() if v.get("intensite", 0) > 0.6]
        if not problematic_patterns:
            return voice_params

        # Adapter les param√®tres vocaux selon les patterns d√©tect√©s
        adapted_params = voice_params.copy()

        # V√©rifier les patterns sp√©cifiques et appliquer des adaptations
        for pattern, info in patterns.items():
            intensity = info.get("intensite", 0)

            if intensity > 0.6:
                if pattern == "oscillation":
                    # Stabiliser la voix pour les patterns d'oscillation
                    adapted_params["stability"] = min(0.9, adapted_params.get("stability", 0.5) + 0.3)
                    adapted_params["rate"] = max(0.9, min(1.1, adapted_params.get("rate", 1.0)))

                elif pattern == "fixation":
                    # Ajouter de la variabilit√© pour les patterns de fixation
                    if "variability" not in adapted_params:
                        adapted_params["variability"] = 0.3
                    else:
                        adapted_params["variability"] += 0.2

                elif pattern == "repli":
                    # Augmenter l√©g√®rement l'√©nergie pour les patterns de repli
                    adapted_params["energy"] = min(0.8, adapted_params.get("energy", 0.5) + 0.2)

                elif pattern == "suractivation":
                    # R√©duire l'√©nergie pour les patterns de suractivation
                    adapted_params["energy"] = max(0.3, adapted_params.get("energy", 0.5) - 0.2)
                    adapted_params["rate"] = max(0.8, adapted_params.get("rate", 1.0) - 0.1)

        self.logger.info(
            f"Param√®tres vocaux adapt√©s pour compenser les patterns √©motionnels: {', '.join(problematic_patterns)}"
        )
        return adapted_params

    except Exception as e:
        self.logger.error(f"Erreur lors de l'ajustement vocal pour les patterns √©motionnels: {e}")
        return voice_params  # En cas d'erreur, retourner les param√®tres originaux


# SPRINT 220: Int√©gration du s√©lecteur de r√¥le √©motionnel


def apply_emotional_role_to_voice(self, voice_params: dict[str, Any], current_emotion: str) -> dict[str, Any]:
    """
    Applique les caract√©ristiques du r√¥le √©motionnel actif aux param√®tres vocaux.
    Impl√©ment√© pour le Sprint 220.

    Args:
        voice_params: Param√®tres vocaux √† adapter
        current_emotion: √âmotion actuelle demand√©e

    Returns:
        Dict[str, Any]: Param√®tres vocaux adapt√©s
    """
    # Si le c≈ìur √©motionnel n'a pas de s√©lecteur de r√¥le, retourner les param√®tres tels quels
    if not self.emotional_core or not hasattr(self.emotional_core, "obtenir_role_actif"):
        return voice_params

    try:
        # V√©rifier si un r√¥le √©motionnel est actif
        role_result = self.emotional_core.obtenir_role_actif()

        if not role_result.get("success", False) or not role_result.get("role_actif", False):
            return voice_params  # Pas de r√¥le actif, aucune modification n√©cessaire

        # R√©cup√©rer les informations du r√¥le
        role_info = role_result.get("role", {})
        role_name = role_info.get("nom", "")
        role_intensity = role_result.get("intensite", 0.7)
        role_traits = role_info.get("traits_vocaux", {})

        if not role_traits:
            return voice_params  # Pas de traits vocaux d√©finis pour ce r√¥le

        # Adapter les param√®tres vocaux selon les traits du r√¥le
        adapted_params = voice_params.copy()

        # Appliquer les traits vocaux selon l'intensit√© du r√¥le
        for trait, value in role_traits.items():
            if trait in adapted_params:
                # M√©langer la valeur actuelle avec celle du r√¥le selon l'intensit√©
                current_value = adapted_params[trait]
                role_value = value
                adapted_params[trait] = current_value * (1 - role_intensity) + role_value * role_intensity
            else:
                # Appliquer directement le trait du r√¥le, ajust√© par l'intensit√©
                adapted_params[trait] = value * role_intensity

        # Si le r√¥le d√©finit une √©motion de base, l'utiliser comme √©motion secondaire
        if "emotion_base" in role_info and role_intensity > 0.4:
            role_emotion = role_info["emotion_base"]

            # Ne pas √©craser l'√©motion primaire, mais ajouter comme secondaire si pertinent
            if current_emotion != role_emotion and "secondary_emotion" not in adapted_params:
                adapted_params["secondary_emotion"] = role_emotion
                adapted_params["secondary_emotion_intensity"] = min(0.7, role_intensity)

        self.logger.info(
            f"Param√®tres vocaux adapt√©s pour le r√¥le √©motionnel '{role_name}' (intensit√©: {role_intensity:.2f})"
        )
        return adapted_params

    except Exception as e:
        self.logger.error(f"Erreur lors de l'application du r√¥le √©motionnel aux param√®tres vocaux: {e}")
    return voice_params  # En cas d'erreur, retourner les param√®tres originaux


# SPRINT 223: Impl√©mentation de la reconnaissance vocale


def start_listening(
    self,
    callback: Callable[[str, float], None] | None = None,
    wake_word: str | None = None,
    continuous: bool | None = None,
    sensitivity: float | None = None,
    language: str | None = None,
) -> bool:
    """
    D√©marre la reconnaissance vocale continue.

    Args:
        callback: Fonction appel√©e lorsqu'un texte est reconnu (texte, confiance)
        wake_word: Mot d'activation pour la reconnaissance (d√©faut: "Jeffrey")
        continuous: Mode d'√©coute continue (attente du mot d'activation)
        sensitivity: Sensibilit√© de la reconnaissance (0.0-1.0)
        language: Code de langue pour la reconnaissance (e.g., "fr-FR")

    Returns:
        bool: True si la reconnaissance a d√©marr√©, False sinon
    """
    # V√©rifier si d√©j√† en √©coute
    if self.listening_active:
        self.logger.info("La reconnaissance vocale est d√©j√† active")
    return True

    # Mettre √† jour les param√®tres locaux si fournis
    if wake_word:
        self.wake_word = wake_word
    if continuous is not None:
        self.continuous_listening = continuous
    if sensitivity is not None:
        self.sensitivity = max(0.0, min(1.0, sensitivity))
    if language:
        self.recognition_language = language
    self.recognition_callback = callback

    try:
        # Importer le module de reconnaissance vocale Jeffrey
        from orchestrateur.recognition.jeffrey_voice_recognition import (
            set_recognition_params,
            start_recognition_loop,
        )

        # Configurer les param√®tres du module de reconnaissance
        set_recognition_params(
            {
                "wake_word": self.wake_word,
                "continuous_listening": self.continuous_listening,
                "sensitivity": self.sensitivity,
                "language": self.recognition_language,
                "callback": self.recognition_callback,
            }
        )

        # D√©marrer la reconnaissance vocale
        start_recognition_loop()

        # Marquer la reconnaissance comme active
        self.listening_active = True

        self.logger.info(
            f"Reconnaissance vocale d√©marr√©e (wake_word={self.wake_word}, continuous={self.continuous_listening})"
        )
        return True

    except ImportError as e:
        self.logger.error(f"Impossible de d√©marrer la reconnaissance vocale: Module manquant - {e}")
        print(f"‚ùå Module requis non trouv√©: {str(e).split('named ')[-1] if 'named' in str(e) else str(e)}")
        print("   V√©rifiez que le module recognition/jeffrey_voice_recognition.py est bien pr√©sent")
        return False
    except Exception as e:
        self.logger.error(f"Erreur lors du d√©marrage de la reconnaissance vocale: {e}")
        return False


def stop_listening(self) -> bool:
    """
    Arr√™te la reconnaissance vocale continue.

    Returns:
        bool: True si la reconnaissance a √©t√© arr√™t√©e, False sinon
    """
    if not self.listening_active:
        self.logger.info("La reconnaissance vocale n'est pas active")
    return True

    try:
        # Importer la fonction pour arr√™ter la reconnaissance
        from orchestrateur.recognition.jeffrey_voice_recognition import stop_recognition_loop

        # Arr√™ter la reconnaissance vocale
        stop_recognition_loop()

        # Marquer la reconnaissance comme inactive
        self.listening_active = False
        self.speech_recognizer = None
        self.speech_recognition_thread = None

        self.logger.info("Reconnaissance vocale arr√™t√©e avec succ√®s")
        return True

    except ImportError as e:
        self.logger.error(f"Impossible d'arr√™ter la reconnaissance vocale: Module manquant - {e}")
        return False
    except Exception as e:
        self.logger.error(f"Erreur lors de l'arr√™t de la reconnaissance vocale: {e}")
        return False


def _listen_thread(self):
    """
    Thread d'√©coute pour la reconnaissance vocale continue.
    Cette m√©thode s'ex√©cute dans un thread s√©par√©.
    """
    import speech_recognition as sr

    # Mode d'attente initial
    waiting_for_wake_word = self.continuous_listening

    self.logger.info("Thread d'√©coute d√©marr√©")
    print("üé§ Reconnaissance vocale active")

    if waiting_for_wake_word:
        print(f"üí° En attente du mot d'activation: '{self.wake_word}'")

    mic = None

    try:
        # Cr√©er le microphone
        mic = sr.Microphone()

        # Effectuer un ajustement initial pour le bruit ambiant
        with mic as source:
            self.logger.info("Ajustement pour le bruit ambiant...")
            self.speech_recognizer.adjust_for_ambient_noise(source, duration=1)
            self.logger.info(f"Seuil d'√©nergie: {self.speech_recognizer.energy_threshold}")

        # Boucle d'√©coute
        while self.listening_active:
            try:
                with mic as source:
                    self.logger.debug("En attente d'audio...")

                    # Annoncer le mode d'√©coute si changement
                    if waiting_for_wake_word:
                        # Afficher une indication visuelle d'attente
                        print("üëÇ ...", end="\r")

                    # √âcouter un segment audio
                    audio = self.speech_recognizer.listen(source, timeout=2, phrase_time_limit=10)

                    # Tenter de reconna√Ætre le texte
                    try:
                        # Utiliser Google Speech Recognition (n√©cessite une connexion internet)
                        text = self.speech_recognizer.recognize_google(audio, language=self.recognition_language)

                        self.logger.debug(f"Texte reconnu: {text}")

                        # Traiter le texte selon le mode d'√©coute
                        if waiting_for_wake_word:
                            # V√©rifier si le mot d'activation est pr√©sent
                            if self.wake_word.lower() in text.lower():
                                print(f"üîä Mot d'activation d√©tect√©: '{self.wake_word}'")
                                waiting_for_wake_word = False

                                # Jouer un son d'activation si disponible
                                self.play_sound_auto("activation")

                                # Message de confirmation
                                if self.voice_mode.upper() == "ON":
                                    self.speak("Je vous √©coute.", "int√©r√™t")
                        else:
                            # Mode √©coute active
                            confidence = 0.8  # Google ne fournit pas la confiance

                            # V√©rifier les commandes sp√©ciales
                            if "arr√™te-toi" in text.lower() or "arr√™te toi" in text.lower():
                                print("üîä Commande d'arr√™t d√©tect√©e")
                                if self.voice_mode.upper() == "ON":
                                    self.speak(
                                        "Je m'arr√™te. Appelez-moi quand vous aurez besoin de moi.",
                                        "neutre",
                                    )
                                waiting_for_wake_word = True
                                continue

                            if "au revoir" in text.lower():
                                print("üîä Commande de fin d√©tect√©e")
                                if self.voice_mode.upper() == "ON":
                                    self.speak(
                                        "Au revoir. J'ai √©t√© ravi de discuter avec vous.",
                                        "amical",
                                    )
                                self.listening_active = False
                                continue

                            # Appeler le callback avec le texte reconnu
                            if self.recognition_callback:
                                self.recognition_callback(text, confidence)
                            else:
                                print(f"üé§ Texte reconnu: {text}")
                                # Si pas de callback, r√©pondre directement
                                if self.emotional_core and hasattr(self.emotional_core, "r√©pondre"):
                                    response = self.emotional_core.r√©pondre(text)
                                    if response and self.voice_mode.upper() == "ON":
                                        self.speak(response, "neutre")

                    except sr.UnknownValueError:
                        self.logger.debug("Impossible de comprendre l'audio")
                    except sr.RequestError as e:
                        self.logger.error(f"Erreur de service de reconnaissance: {e}")
                        print("‚ö†Ô∏è Erreur de service de reconnaissance. V√©rifiez votre connexion internet.")
                        # Pause avant nouvelle tentative
                        time.sleep(3)

            except Exception as loop_error:
                self.logger.error(f"Erreur dans la boucle d'√©coute: {loop_error}")
                # Pause avant nouvelle tentative
                time.sleep(2)

    except Exception as thread_error:
        self.logger.error(f"Erreur critique dans le thread d'√©coute: {thread_error}")
    finally:
        self.logger.info("Thread d'√©coute termin√©")
        print("üîá Reconnaissance vocale d√©sactiv√©e")


# SPRINT 221: Int√©gration du narrateur d'√©tat interne


def add_internal_state_narration(self, response: str, add_prefix: bool = False) -> str:
    """
    Ajoute une narration d'√©tat interne √† une r√©ponse textuelle si n√©cessaire.
    Impl√©ment√© pour le Sprint 221.

    Args:
        response: R√©ponse textuelle originale
        add_prefix: Ajouter la narration au d√©but de la r√©ponse plut√¥t qu'√† la fin

    Returns:
        str: R√©ponse avec narration d'√©tat interne
    """
    # Si le c≈ìur √©motionnel n'a pas de narrateur d'√©tat interne, retourner la r√©ponse telle quelle
    if not self.emotional_core or not hasattr(self.emotional_core, "verbaliser_etat_interne"):
        return response

    try:
        # Obtenir les pr√©f√©rences actuelles de narration
        prefs_result = self.emotional_core.obtenir_preferences_narration()

        if not prefs_result.get("success", False):
            return response  # Impossible d'obtenir les pr√©f√©rences, aucune modification

        # V√©rifier si la narration est activ√©e et si elle doit √™tre incluse dans les r√©ponses
        frequence = prefs_result.get("frequence", "jamais")
        inclure_dans_reponse = prefs_result.get("inclure_dans_reponse", False)

        if frequence == "jamais" or not inclure_dans_reponse:
            return response  # Narration d√©sactiv√©e ou ne doit pas √™tre incluse

        # Obtenir la verbalisation de l'√©tat interne
        narration_result = self.emotional_core.verbaliser_etat_interne(
            type_verbalisation=prefs_result.get("type_defaut", "bref"),
            style=prefs_result.get("style_defaut", "naturel"),
            inclure_dans_reponse=True,
        )

        if not narration_result.get("success", False) or not narration_result.get("verbalisation"):
            return response  # Pas de verbalisation disponible

        narration_text = narration_result.get("verbalisation", "")

        # D√©terminer si la verbalisation doit √™tre g√©n√©r√©e selon la fr√©quence configur√©e
        if frequence == "rare" and random.random() > 0.2:
            return response
        if frequence == "occasionnel" and random.random() > 0.5:
            return response

        # Ajouter la narration √† la r√©ponse
        if add_prefix:
            # Ajouter au d√©but avec une s√©paration claire
            return f"{narration_text}\n\n{response}"
        else:
            # Ajouter √† la fin avec une s√©paration claire
            return f"{response}\n\n{narration_text}"

    except Exception as e:
        self.logger.error(f"Erreur lors de l'ajout de la narration d'√©tat interne: {e}")
        return response  # En cas d'erreur, retourner la r√©ponse originale


# SPRINT 222+: Int√©gration du moteur d'adaptation de personnalit√©


def adapt_voice_to_personality(self, voice_params: dict[str, Any], current_emotion: str) -> dict[str, Any]:
    """
    Adapte les param√®tres vocaux en fonction du profil de personnalit√© actuel.
    Impl√©ment√© pour le syst√®me d'adaptation de personnalit√©.

    Args:
        voice_params: Param√®tres vocaux √† adapter
        current_emotion: √âmotion actuelle demand√©e

    Returns:
        Dict[str, Any]: Param√®tres vocaux adapt√©s
    """
    # Si le moteur d'adaptation de personnalit√© n'est pas disponible, retourner les param√®tres tels quels
    if not self.adaptive_personality_engine:
        return voice_params

    try:
        # Obtenir le style actuel de la personnalit√©
        current_style = self.adaptive_personality_engine.get_current_style()

        # Si aucun style n'est disponible, retourner les param√®tres tels quels
        if not current_style:
            return voice_params

        # Adapter les param√®tres vocaux selon le style de personnalit√©
        adapted_params = voice_params.copy()

        # Appliquer les traits vocaux du style de personnalit√©
        if "vocal_style" in current_style:
            vocal_style = current_style["vocal_style"]

            # Adapter le rythme vocal
            if "pace" in vocal_style and "rate" in adapted_params:
                # Convertir pace (0-1) en rate (0.8-1.2 typiquement)
                pace = vocal_style["pace"]
                adapted_params["rate"] = 0.8 + (pace * 0.4)  # 0 -> 0.8, 1 -> 1.2

            # Note: pitch_variation n'est pas support√© par l'API d'ElevenLabs
            # Suppression de la r√©f√©rence √† pitch_variation

            # Adapter l'expressivit√©
            if "expressivity" in vocal_style and "expression" in adapted_params:
                adapted_params["expression"] = vocal_style["expressivity"]

            # Adapter la douceur
            if "softness" in vocal_style:
                softness = vocal_style["softness"]
                # Appliquer la douceur √† plusieurs param√®tres
                if "smoothness" in adapted_params:
                    adapted_params["smoothness"] = softness
                if "breathiness" in adapted_params:
                    adapted_params["breathiness"] = 0.3 + (softness * 0.4)  # 0 -> 0.3, 1 -> 0.7

        # Appliquer les traits de ton qui peuvent affecter la voix
        if "tone" in current_style:
            tone = current_style["tone"]

            # La chaleur peut affecter la qualit√© vocale
            if "warmth" in tone:
                warmth = tone["warmth"]
                if "warmth" in adapted_params:
                    adapted_params["warmth"] = warmth
                elif "timbre" in adapted_params:
                    # Plus chaud = timbre plus riche
                    adapted_params["timbre"] = 0.4 + (warmth * 0.3)  # 0 -> 0.4, 1 -> 0.7

            # La formalit√© peut affecter la pr√©cision d'articulation
            if "formality" in tone and "articulation" in adapted_params:
                formality = tone["formality"]
                adapted_params["articulation"] = 0.5 + (formality * 0.4)  # 0 -> 0.5, 1 -> 0.9

        # Enregistrer une interaction pour le moteur d'adaptation si pertinent
        self._log_voice_interaction_for_adaptation(current_emotion, adapted_params)

        self.logger.info("Param√®tres vocaux adapt√©s selon le profil de personnalit√© actuel")
        return adapted_params

    except Exception as e:
        self.logger.error(f"Erreur lors de l'adaptation vocale au profil de personnalit√©: {e}")
        return voice_params  # En cas d'erreur, retourner les param√®tres originaux


def _log_voice_interaction_for_adaptation(self, emotion: str, voice_params: dict[str, Any]) -> None:
    """
    Enregistre les donn√©es d'interaction vocale pour le moteur d'adaptation.

    Args:
        emotion: √âmotion utilis√©e pour la synth√®se
        voice_params: Param√®tres vocaux utilis√©s
    """
    if not self.adaptive_personality_engine:
        return

    try:
        # Pr√©parer les donn√©es d'interaction pour le moteur d'adaptation
        from datetime import datetime

        interaction_data = {
            "timestamp": datetime.now().isoformat(),
            "voice": {
                "emotion": emotion,
                "pace_preference": (
                    voice_params.get("rate", 0.5) if "rate" in voice_params else voice_params.get("pace", 0.5)
                ),
                "tone_feedback": voice_params.get("emphasis", 0.5),
            },
        }

        # Observer l'interaction pour adaptation future
        self.adaptive_personality_engine.observe_interaction(interaction_data)

    except Exception as e:
        self.logger.error(f"Erreur lors de l'enregistrement de l'interaction vocale pour l'adaptation: {e}")


def process_voice_params_for_synthesis(self, text: str, emotion: str, voice_params: dict[str, Any]) -> dict[str, Any]:
    """
    Traite les param√®tres vocaux pour la synth√®se en appliquant toutes les adaptations.
    Cette m√©thode applique successivement toutes les couches d'adaptation:
    1. Adaptation de personnalit√©
    2. Stabilit√© √©motionnelle
    3. Patterns √©motionnels
    4. R√¥le √©motionnel

    Args:
        text: Texte √† synth√©tiser
        emotion: √âmotion principale
        voice_params: Param√®tres vocaux initiaux

    Returns:
        Dict[str, Any]: Param√®tres vocaux finaux apr√®s toutes les adaptations
    """
    processed_params = voice_params.copy()

    # √âtape 1: Adapter selon le profil de personnalit√©
    processed_params = self.adapt_voice_to_personality(processed_params, emotion)

    # √âtape 2: Appliquer les contraintes de stabilit√© √©motionnelle
    processed_params = self.apply_emotional_stability_to_voice(processed_params, emotion)

    # √âtape 3: Ajuster pour les patterns √©motionnels probl√©matiques
    processed_params = self.adjust_voice_for_emotional_patterns(processed_params, emotion)

    # √âtape 4: Appliquer les caract√©ristiques du r√¥le √©motionnel actif
    processed_params = self.apply_emotional_role_to_voice(processed_params, emotion)

    return processed_params
