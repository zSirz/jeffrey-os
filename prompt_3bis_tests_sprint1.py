#!/usr/bin/env python3
"""
JEFFREY OS - PROMPT 3-BIS : TESTS SPRINT 1 - VALIDATION COMPL√àTE
=================================================================

Ce script cr√©e et lance le runner de tests Sprint 1 qui :
- Utilise EmotionDetectorV2 sur les 40 sc√©narios YAML
- Calcule Macro-F1, Accuracy, Confusion Matrix
- Mesure la latence (avg, p95)
- G√©n√®re un rapport JSON d√©taill√©

OBJECTIF SPRINT 1 :
- Macro-F1 ‚â• 0.70
- Accuracy ‚â• 0.65
- Latence p95 ‚â§ 500ms

USAGE:
    python3 prompt_3bis_tests_sprint1.py

√âquipe : Dream Team Jeffrey OS
"""

import os
import subprocess
import sys
from pathlib import Path

# ===============================================================================
# CONFIGURATION
# ===============================================================================

PROJECT_ROOT = Path(__file__).parent
TESTS_DIR = PROJECT_ROOT / "tests"
RESULTS_DIR = PROJECT_ROOT / "test_results"

print("=" * 80)
print("üöÄ JEFFREY OS - PROMPT 3-BIS : TESTS SPRINT 1")
print("=" * 80)
print()
print("Ce script va :")
print("1. Cr√©er le runner de tests Sprint 1")
print("2. Lancer les 40 sc√©narios YAML")
print("3. Calculer Macro-F1, Accuracy, Confusion Matrix")
print("4. Mesurer la latence (avg, p95)")
print("5. G√©n√©rer le rapport JSON")
print()

# ===============================================================================
# √âTAPE 1 : CR√âATION DU RUNNER SPRINT 1
# ===============================================================================

print("üìù [1/2] Cr√©ation du runner de tests Sprint 1...")
print()

runner_code = '''#!/usr/bin/env python3
"""
JEFFREY OS - Runner Sprint 1 : √âvaluation D√©tection √âmotionnelle
================================================================

Ce runner √©value EmotionDetectorV2 sur les 40 sc√©narios YAML.

M√©triques calcul√©es :
- Macro-F1 par √©motion
- Accuracy globale
- Confusion Matrix
- Latence (avg, p95)

√âquipe : Dream Team Jeffrey OS (impl√©mentation GPT/Marc)
"""

import os
import sys
import time
import json
from pathlib import Path
from dataclasses import dataclass
from collections import Counter, defaultdict
import yaml

# Assure imports locaux
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))

# Modules Sprint 1
from jeffrey.nlp.emotion_detector_v2 import EmotionDetectorV2


@dataclass
class Example:
    """Exemple de pr√©diction pour √©valuation"""
    scenario: str
    turn_idx: int
    text: str
    gold: str
    pred: str
    latency_ms: float


def load_scenarios(dirpath: Path):
    """
    Charge tous les fichiers YAML du dossier.

    Supporte 2 sch√©mas:
    - Nouveau: {metadata, conversation: [{role, content, expected_emotion?}], validation?}
    - Ancien : {meta, session: [{user: "...", assert_* / expect_* ...}]}
    """
    files = sorted(dirpath.glob("*.yaml"))
    scenarios = []

    for f in files:
        try:
            data = yaml.safe_load(f.read_text(encoding='utf-8'))
            scenarios.append((f.name, data))
        except Exception as e:
            print(f"‚ö†Ô∏è  YAML invalide {f.name}: {e}")

    return scenarios


def iterate_user_turns(name, data):
    """
    It√®re sur les tours utilisateur annot√©s.

    Yields:
        (turn_idx, text, gold_emotion)
    """
    # Nouveau sch√©ma
    if "conversation" in data:
        for i, step in enumerate(data["conversation"]):
            if step.get("role") == "user":
                text = step.get("content", "")
                gold = step.get("expected_emotion", None)
                yield i, text, gold
        return

    # Ancien sch√©ma
    if "session" in data:
        for i, step in enumerate(data["session"]):
            if "user" in step:
                text = step.get("user", "")
                # Pas toujours d'annotation ‚Üí None
                gold = step.get("expected_emotion", None) or step.get("expect_emotion", None)
                yield i, text, gold
        return


def compute_metrics(examples):
    """
    Calcule Macro-F1, Accuracy, Confusion Matrix.

    Args:
        examples: Liste d'Example avec gold et pred

    Returns:
        Dict avec m√©triques
    """
    # R√©cup√©rer toutes les √©motions gold uniques
    labels = sorted(set(ex.gold for ex in examples if ex.gold))

    if not labels:
        return {
            "note": "Aucune √©tiquette de v√©rit√© terrain trouv√©e dans les YAML.",
            "macro_f1": 0.0,
            "accuracy": 0.0,
            "support": 0,
            "confusion": {},
        }

    # Mapping label ‚Üí index
    idx = {l: i for i, l in enumerate(labels)}

    # Confusion matrix
    conf = [[0 for _ in labels] for __ in labels]
    support = Counter()
    correct = 0

    for ex in examples:
        if not ex.gold:
            continue

        support[ex.gold] += 1

        if ex.pred == ex.gold:
            correct += 1

        # Si la pr√©d n'est pas dans labels (ex: "neutral"), on l'ignore pour confusion
        if ex.pred in idx and ex.gold in idx:
            conf[idx[ex.gold]][idx[ex.pred]] += 1

    # F1 par label
    def f1_for(label):
        i = idx[label]
        tp = conf[i][i]
        fp = sum(conf[r][i] for r in range(len(labels)) if r != i)
        fn = sum(conf[i][c] for c in range(len(labels)) if c != i)

        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0

        return (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

    per_label_f1 = {l: round(f1_for(l), 4) for l in labels}
    macro_f1 = round(sum(per_label_f1.values()) / len(labels), 4)
    acc = round(correct / sum(support.values()), 4) if support else 0.0

    # Confusion matrix lisible
    confusion = {
        "labels": labels,
        "matrix": conf,
    }

    return {
        "macro_f1": macro_f1,
        "accuracy": acc,
        "support": sum(support.values()),
        "per_label_f1": per_label_f1,
        "per_label_support": dict(support),
        "confusion": confusion,
    }


def main():
    """Point d'entr√©e principal"""
    base = ROOT
    conv_dir = base / "tests" / "convos"

    if not conv_dir.exists():
        print("‚ö†Ô∏è  tests/convos/ introuvable. Cr√©e d'abord les sc√©narios.")
        sys.exit(1)

    # Initialiser le d√©tecteur
    det = EmotionDetectorV2()
    scenarios = load_scenarios(conv_dir)

    all_examples = []
    latencies = []

    print("=" * 80)
    print("üß™ JEFFREY OS ‚Äî Sprint 1 Emotion Eval Runner")
    print("=" * 80)
    print()
    print(f"üìÅ Dossier : {conv_dir}")
    print(f"üìä Sc√©narios d√©tect√©s : {len(scenarios)}")
    print()
    print("‚è≥ Traitement en cours...")
    print()

    # Traiter tous les sc√©narios
    for name, data in scenarios:
        for turn_idx, text, gold in iterate_user_turns(name, data):
            if not text.strip():
                continue

            # D√©tection avec mesure de latence
            t0 = time.perf_counter()
            res = det.detect(text)
            dt = (time.perf_counter() - t0) * 1000.0

            latencies.append(dt)
            pred = res.primary

            all_examples.append(
                Example(name, turn_idx, text, gold, pred, dt)
            )

    # Garder uniquement les exemples annot√©s pour le calcul F1
    annotated = [ex for ex in all_examples if ex.gold]

    print(f"‚úÖ Traitement termin√© : {len(all_examples)} tours, {len(annotated)} annot√©s")
    print()

    # Calculer les m√©triques
    metrics = compute_metrics(annotated)

    # Latence
    lat_sorted = sorted(latencies)
    p95 = round(lat_sorted[int(0.95 * len(lat_sorted)) - 1], 2) if lat_sorted else 0.0
    avg = round(sum(latencies) / len(latencies), 2) if latencies else 0.0

    # Rapport complet
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "num_scenarios": len(scenarios),
        "num_turns": len(all_examples),
        "num_annotated": len(annotated),
        "latency_ms": {
            "avg": avg,
            "p95": p95,
            "min": round(min(latencies), 2) if latencies else 0.0,
            "max": round(max(latencies), 2) if latencies else 0.0
        },
        "emotion_metrics": metrics,
        "samples": [
            {
                "scenario": ex.scenario,
                "turn_idx": ex.turn_idx,
                "gold": ex.gold,
                "pred": ex.pred,
                "match": "‚úÖ" if ex.gold == ex.pred else "‚ùå",
                "latency_ms": round(ex.latency_ms, 2),
                "text": ex.text[:200]
            } for ex in annotated[:25]  # √âchantillon
        ]
    }

    # Sauvegarder le rapport
    out_dir = ROOT / "test_results"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_json = out_dir / "sprint1_emotion_eval.json"
    out_json.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding='utf-8')

    # Afficher le r√©sum√©
    print("=" * 80)
    print("üìä R√âSULTATS SPRINT 1")
    print("=" * 80)
    print()
    print(f"üìà M√âTRIQUES GLOBALES :")
    print(f"   ‚Ä¢ Tours annot√©s : {report['num_annotated']} / {report['num_turns']}")
    print(f"   ‚Ä¢ Macro-F1      : {metrics.get('macro_f1', 0.0):.3f}")
    print(f"   ‚Ä¢ Accuracy      : {metrics.get('accuracy', 0.0):.3f}")
    print()
    print(f"‚ö° LATENCE :")
    print(f"   ‚Ä¢ Moyenne  : {avg}ms")
    print(f"   ‚Ä¢ P95      : {p95}ms")
    print(f"   ‚Ä¢ Min/Max  : {report['latency_ms']['min']}ms / {report['latency_ms']['max']}ms")
    print()

    # F1 par √©motion
    if "per_label_f1" in metrics:
        print(f"üé≠ F1 PAR √âMOTION :")
        for label, f1 in sorted(metrics["per_label_f1"].items(), key=lambda x: -x[1]):
            support = metrics.get("per_label_support", {}).get(label, 0)
            print(f"   ‚Ä¢ {label:15s} : {f1:.3f}  (n={support})")
        print()

    # Confusion matrix
    if "labels" in metrics.get("confusion", {}):
        labels = metrics["confusion"]["labels"]
        matrix = metrics["confusion"]["matrix"]
        print(f"üîÄ CONFUSION MATRIX :")
        print(f"   Labels : {', '.join(labels)}")
        print()

    print(f"üíæ RAPPORT SAUVEGARD√â :")
    print(f"   {out_json}")
    print()

    # Verdict Sprint 1
    macro_f1 = metrics.get("macro_f1", 0.0)
    accuracy = metrics.get("accuracy", 0.0)

    print("=" * 80)
    print("üéØ VERDICT SPRINT 1")
    print("=" * 80)
    print()

    if macro_f1 >= 0.70 and p95 <= 500:
        print("‚úÖ OBJECTIFS ATTEINTS !")
        print(f"   ‚Ä¢ Macro-F1 : {macro_f1:.3f} (objectif ‚â• 0.70) ‚úÖ")
        print(f"   ‚Ä¢ Latence  : {p95}ms (objectif ‚â§ 500ms) ‚úÖ")
        print()
        print("üöÄ PR√äT POUR PROMPT 4 : Int√©gration Jeffrey OS !")
    elif macro_f1 >= 0.65:
        print("‚ö†Ô∏è  PROCHE DE L'OBJECTIF")
        print(f"   ‚Ä¢ Macro-F1 : {macro_f1:.3f} (objectif ‚â• 0.70) ‚ö†Ô∏è")
        print(f"   ‚Ä¢ Latence  : {p95}ms (objectif ‚â§ 500ms) {'‚úÖ' if p95 <= 500 else '‚ùå'}")
        print()
        print("üí° Quelques ajustements n√©cessaires avant int√©gration.")
    else:
        print("‚ùå OBJECTIFS NON ATTEINTS")
        print(f"   ‚Ä¢ Macro-F1 : {macro_f1:.3f} (objectif ‚â• 0.70) ‚ùå")
        print(f"   ‚Ä¢ Latence  : {p95}ms (objectif ‚â§ 500ms) {'‚úÖ' if p95 <= 500 else '‚ùå'}")
        print()
        print("üîß Besoin d'am√©liorer le lexique ou les patterns.")

    print()
    print("=" * 80)


if __name__ == "__main__":
    main()
'''

# Cr√©er le dossier tests si n√©cessaire
TESTS_DIR.mkdir(exist_ok=True)

# √âcrire le fichier
runner_file = TESTS_DIR / "runner_convos_sprint1.py"
with open(runner_file, 'w', encoding='utf-8') as f:
    f.write(runner_code)

# Rendre ex√©cutable
os.chmod(runner_file, 0o755)

print(f"‚úÖ Fichier cr√©√© : {runner_file}")
print()

# ===============================================================================
# √âTAPE 2 : EX√âCUTION DES TESTS
# ===============================================================================

print("üß™ [2/2] Lancement des tests Sprint 1...")
print()
print("‚è≥ Cela peut prendre 30-60 secondes...")
print()

# Ex√©cuter avec PYTHONPATH correct
env = os.environ.copy()
env['PYTHONPATH'] = str(PROJECT_ROOT / "src")

try:
    result = subprocess.run(
        [sys.executable, str(runner_file)],
        cwd=PROJECT_ROOT,
        env=env,
        capture_output=True,
        text=True,
        timeout=120,  # 2 minutes max
    )

    # Afficher la sortie
    print(result.stdout)

    if result.stderr:
        print("‚ö†Ô∏è  Warnings/Errors :")
        print(result.stderr)

    if result.returncode != 0:
        print(f"‚ùå Le runner a retourn√© un code d'erreur : {result.returncode}")
        sys.exit(1)

except subprocess.TimeoutExpired:
    print("‚ùå Timeout : Les tests ont pris trop de temps (>2 min)")
    sys.exit(1)
except Exception as e:
    print(f"‚ùå Erreur lors de l'ex√©cution : {e}")
    sys.exit(1)

# ===============================================================================
# √âTAPE 3 : AFFICHAGE DU RAPPORT
# ===============================================================================

print()
print("=" * 80)
print("üìÑ RAPPORT JSON DISPONIBLE")
print("=" * 80)
print()

report_file = RESULTS_DIR / "sprint1_emotion_eval.json"

if report_file.exists():
    print(f"‚úÖ Rapport sauvegard√© : {report_file}")
    print()
    print("üìä Pour voir les d√©tails :")
    print(f"   cat {report_file}")
    print()
    print("   ou ouvre-le dans VS Code pour explorer les m√©triques.")
else:
    print("‚ö†Ô∏è  Le rapport JSON n'a pas √©t√© g√©n√©r√©.")

print()
print("=" * 80)
print("‚úÖ PROMPT 3-BIS TERMIN√â !")
print("=" * 80)
print()
print("üéØ PROCHAINE √âTAPE :")
print("   Si Macro-F1 ‚â• 0.70 ‚Üí PROMPT 4 (Int√©gration Jeffrey OS)")
print("   Si Macro-F1 < 0.70 ‚Üí Am√©liorer le lexique et relancer")
print()
